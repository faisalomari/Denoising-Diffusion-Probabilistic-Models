{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MWoe14Ai3I-"
      },
      "source": [
        "# Exercise 7: Denoising Diffusion Probabilistic Models\n",
        "\n",
        "Submitted by:\n",
        "\n",
        " ** Faisal Omari 325616894\n",
        "\n",
        " ** Fadi Khatib 308052992"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA5GrP2zY9bI"
      },
      "source": [
        "### Load libraries and utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lzI4F5xRY9bK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/faisal/anaconda3/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/faisal/anaconda3/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c107WarningC1ENS_7variantIJNS0_11UserWarningENS0_18DeprecationWarningEEEERKNS_14SourceLocationENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEb'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n",
            "Seed set to 42\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device cuda:0\n"
          ]
        }
      ],
      "source": [
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import os\n",
        "\n",
        "# Visualization tools\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip install --quiet pytorch-lightning>=1.4\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
        "print(\"Using device\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VEjj8YMDY9bL"
      },
      "outputs": [],
      "source": [
        "def show_tensor_image(img):\n",
        "    reverse_transforms = transforms.Compose([\n",
        "        transforms.Lambda(lambda t: (t + 1) / 2),\n",
        "        transforms.Lambda(lambda t: torch.minimum(torch.tensor([1]), t)),\n",
        "        transforms.Lambda(lambda t: torch.maximum(torch.tensor([0]), t)),\n",
        "        transforms.ToPILImage(),\n",
        "    ])\n",
        "    plt.imshow(reverse_transforms(img))\n",
        "\n",
        "def trim_imgs(imgs, skip=10):\n",
        "    imgs = imgs.view((-1,) + imgs.shape[2:])        # BATCH * T, CH, WIDTH, HEIGHT\n",
        "    imgs = imgs[::skip]\n",
        "    return imgs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "renhtvkaZBGA"
      },
      "source": [
        "### Load MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_KOpH4ZKjPIC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:24<00:00, 410897.68it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 156506.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:04<00:00, 402359.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 418784.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "IMG_SIZE = 28\n",
        "IMG_CH = 1\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "def load_MNIST(data_transform, train=True):\n",
        "    return torchvision.datasets.MNIST(\n",
        "        \"./\",\n",
        "        download=True,\n",
        "        train=train,\n",
        "        transform=data_transform,\n",
        "    )\n",
        "\n",
        "\n",
        "def load_transformed_MNIST():\n",
        "    data_transforms = [\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor(),  # Scales data into [0,1]\n",
        "        transforms.Lambda(lambda t: (t * 2) - 1)  # Scale between [-1, 1]\n",
        "    ]\n",
        "\n",
        "    data_transform = transforms.Compose(data_transforms)\n",
        "    train_set = load_MNIST(data_transform, train=True)\n",
        "    test_set = load_MNIST(data_transform, train=False)\n",
        "    return train_set, test_set\n",
        "\n",
        "train, test = load_transformed_MNIST()\n",
        "train_dataloader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "test_dataloader = DataLoader(test, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "O6sLfpUfZBGL"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def show_imgs(imgs, nrows=1, grid=True):\n",
        "    ncols = len(imgs) // nrows\n",
        "\n",
        "    imgs = torch.stack(imgs) if isinstance(imgs, list) else imgs\n",
        "    imgs = imgs.unsqueeze(1) if imgs.dim() == 3 else imgs\n",
        "\n",
        "    if grid:\n",
        "        grid = torchvision.utils.make_grid(imgs.cpu(), nrow=ncols, pad_value=128)\n",
        "        show_tensor_image(grid.detach().cpu())\n",
        "    else:\n",
        "        ncols = len(imgs) // nrows\n",
        "        for idx, img in enumerate(imgs):\n",
        "            plt.subplot(nrows, ncols, idx + 1)\n",
        "            plt.axis('off')\n",
        "            show_tensor_image(img.detach().cpu())\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "show_imgs([train[i][0] for i in range(8)], nrows=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5ErXXVQqSye"
      },
      "source": [
        "## Define the architercture\n",
        "\n",
        "<img width=\"70%\" src=\"https://sharon.srworkspace.com/dgm/time.png\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "hKrRnXWfrfOu"
      },
      "outputs": [],
      "source": [
        "#@title Net components\n",
        "\n",
        "class EmbedBlock(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim):\n",
        "        super(EmbedBlock, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        layers = [\n",
        "            nn.Linear(input_dim, emb_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(emb_dim, emb_dim),\n",
        "            nn.Unflatten(1, (emb_dim, 1, 1)),\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.input_dim)\n",
        "        #x = x[:, None]\n",
        "        return self.model(x)\n",
        "\n",
        "class SinusoidalPositionEmbedBlock(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings\n",
        "\n",
        "class DownBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(in_chs, out_chs, 3, 1, 1),\n",
        "            nn.BatchNorm2d(out_chs),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_chs, out_chs, 3, 1, 1),\n",
        "            nn.BatchNorm2d(out_chs),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.ConvTranspose2d(2 * in_chs, out_chs, 3, 2, 1, 1),\n",
        "            nn.BatchNorm2d(out_chs),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_chs, out_chs, 3, 1, 1),\n",
        "            nn.BatchNorm2d(out_chs),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x = torch.cat((x, skip), 1)\n",
        "        x = self.model(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wv8_SKFBsaST"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "incomplete input (2711112174.py, line 60)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 60\u001b[0;36m\u001b[0m\n\u001b[0;31m    # Implement here\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, T):\n",
        "        super(UNet, self).__init__()\n",
        "        self.T = T\n",
        "\n",
        "        img_chs = IMG_CH\n",
        "        down_chs = (64, 128, 128)\n",
        "        up_chs = down_chs[::-1]  # Reverse of the down channels\n",
        "        latent_image_size = IMG_SIZE // 4 # 2 ** (len(down_chs) - 1)\n",
        "\n",
        "        # New\n",
        "        t_embed_dim = 8\n",
        "\n",
        "        # Inital convolution\n",
        "        self.down0 = nn.Sequential(\n",
        "            nn.Conv2d(img_chs, down_chs[0], 3, padding=1),\n",
        "            nn.BatchNorm2d(down_chs[0]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Downsample\n",
        "        self.down1 = DownBlock(down_chs[0], down_chs[1])\n",
        "        self.down2 = DownBlock(down_chs[1], down_chs[2])\n",
        "        self.to_vec = nn.Sequential(nn.Flatten(), nn.ReLU())\n",
        "\n",
        "        # Embeddings\n",
        "        self.dense_emb = nn.Sequential(\n",
        "            nn.Linear(down_chs[2]*latent_image_size**2, down_chs[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(down_chs[1], down_chs[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(down_chs[1], down_chs[2]*latent_image_size**2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.sinusoidaltime = SinusoidalPositionEmbedBlock(t_embed_dim) # New\n",
        "        self.temb_1 = EmbedBlock(t_embed_dim, up_chs[0]) # New\n",
        "        self.temb_2 = EmbedBlock(t_embed_dim, up_chs[1]) # New\n",
        "\n",
        "        # Upsample\n",
        "        self.up0 = nn.Sequential(\n",
        "            nn.Unflatten(1, (up_chs[0], latent_image_size, latent_image_size)),\n",
        "            nn.Conv2d(up_chs[0], up_chs[0], 3, padding=1),\n",
        "            nn.BatchNorm2d(up_chs[0]),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.up1 = UpBlock(up_chs[0], up_chs[1])\n",
        "        self.up2 = UpBlock(up_chs[1], up_chs[2])\n",
        "\n",
        "        # Match output channels\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Conv2d(2 * up_chs[2], up_chs[2], 3, 1, 1),\n",
        "            nn.BatchNorm2d(up_chs[-1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(up_chs[-1], img_chs, 3, 1, 1)\n",
        "        )\n",
        "\n",
        "        print(\"Net Num params: \", sum(p.numel() for p in self.parameters()))\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        # Implement here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVdI8FjrZgRx"
      },
      "source": [
        "## Define DDPM model\n",
        "\n",
        "A fundemantal idea of diffusion models is to add a little noise to the image each time step and learn how to remove it, depending on time. Here, we will use variance schedule.\n",
        "\n",
        "<img width=\"70%\" src=\"https://sharon.srworkspace.com/dgm/dog.png\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caQug2W_ZgRz"
      },
      "outputs": [],
      "source": [
        "class DDPM(pl.LightningModule):\n",
        "    def __init__(self, T=1000, method='cosine'):\n",
        "        super(DDPM, self).__init__()\n",
        "        self.T = T\n",
        "\n",
        "        epsilon=0.008\n",
        "        if method == 'cosine':\n",
        "            steps=torch.linspace(0,T,steps=T+1).to(device)\n",
        "            f_t=torch.cos(((steps/T+epsilon)/(1.0+epsilon))*math.pi*0.5)**2\n",
        "            self.Beta = torch.clip(1.0-f_t[1:]/f_t[:T], 0.0, 0.999)\n",
        "\n",
        "        elif method == 'linear':\n",
        "            self.Beta = torch.linspace(1e-4, 2e-2, T).to(device)\n",
        "\n",
        "        # Forward diffusion variables\n",
        "        self.a = 1.0 - self.Beta\n",
        "        self.a_bar = torch.cumprod(self.a, dim=0)\n",
        "\n",
        "        self.net = UNet(T)\n",
        "\n",
        "        # Logging\n",
        "        self.train_loss = []\n",
        "        self.train_loss_in_epoch = []\n",
        "\n",
        "        self.validation_loss = []\n",
        "        self.validation_loss_in_epoch = []\n",
        "\n",
        "    def q(self, x_0, t):\n",
        "        \"\"\"\n",
        "        Samples a new image from q\n",
        "        Returns the noise applied to an image at timestep t\n",
        "        x_0: the original image\n",
        "        t: timestep\n",
        "        \"\"\"\n",
        "        # Implement here\n",
        "\n",
        "    # To be used later\n",
        "    def get_x0_pred(self, x_t, t, e_t):\n",
        "      x_0_pred = (x_t - torch.sqrt(1 - self.a_bar[t]) * e_t) / torch.sqrt(self.a_bar[t])\n",
        "      x_0_pred.clamp_(-1, 1)\n",
        "      return x_0_pred\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def reverse_q(self, x_t, t, e_t):\n",
        "        # Implement here\n",
        "\n",
        "    def get_loss(self, x_0, t):\n",
        "        \"\"\"\n",
        "        Returns the loss between the true noise and the predicted noise\n",
        "        x_0: the original image\n",
        "        t: timestep\n",
        "        \"\"\"\n",
        "        # Implement here\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, num_imgs=1):\n",
        "\n",
        "        output = torch.zeros((num_imgs, T, IMG_CH, IMG_SIZE, IMG_SIZE))     # For each image, save all the timesteps\n",
        "        # Implement here\n",
        "        return output\n",
        "\n",
        "    # Lightning Configurations\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return Adam(self.parameters(), lr=1e-3)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x = batch[0].to(device)\n",
        "        t = torch.randint(0, self.T, (BATCH_SIZE,), device=device)\n",
        "\n",
        "        loss = self.get_loss(x, t)\n",
        "        self.train_loss_in_epoch.append(loss.item())\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x = batch[0].to(device)\n",
        "        t = torch.randint(0, self.T, (BATCH_SIZE,), device=device)\n",
        "\n",
        "        loss = self.get_loss(x, t)\n",
        "        self.validation_loss_in_epoch.append(loss.item())\n",
        "        return loss\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        avg_loss = torch.tensor(self.train_loss_in_epoch).mean()\n",
        "        self.train_loss_in_epoch = []\n",
        "\n",
        "        self.log(\"train_loss\", avg_loss, prog_bar=True)\n",
        "        self.train_loss.append(avg_loss.detach().item())\n",
        "\n",
        "        print(f\"Epoch {self.current_epoch} | Loss: {avg_loss.detach().item()} \")\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        avg_loss = torch.tensor(self.validation_loss_in_epoch).mean()\n",
        "        self.validation_loss_in_epoch = []\n",
        "\n",
        "        self.log(\"val_loss\", avg_loss, prog_bar=True)\n",
        "        self.validation_loss.append(avg_loss.detach().item())\n",
        "\n",
        "        sampled_data = trim_imgs(self.sample(), skip=100)\n",
        "        show_imgs(sampled_data, grid=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UC3z8ECxZgR0"
      },
      "outputs": [],
      "source": [
        "#@title you may run this to make sure your implementation for q is good\n",
        "\n",
        "x_0 = train[1][0].to(device)  # Initial image\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "ddpm = DDPM(T=1000)\n",
        "\n",
        "for t in range(T):\n",
        "    t_tenser = torch.Tensor([t]).type(torch.int64)\n",
        "    x_t, noise = ddpm.q(x_0, t_tenser)\n",
        "    img = torch.squeeze(x_t).cpu()\n",
        "    if t % 100 == 0:\n",
        "      ax = plt.subplot(1, 10, t // 100 + 1)\n",
        "      ax.axis('off')\n",
        "      show_tensor_image(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOh7AyR5Zd8Z"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsWbEjaR2m6r"
      },
      "outputs": [],
      "source": [
        "def train_model(**kwargs):\n",
        "    # Create a PyTorch Lightning trainer with the generation callback\n",
        "    trainer = pl.Trainer(\n",
        "        default_root_dir=os.path.join(\"DDPM\"),\n",
        "        devices=1,\n",
        "        max_epochs=20,\n",
        "        callbacks=[\n",
        "            ModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"val_loss\"),\n",
        "            LearningRateMonitor(\"epoch\")\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model = DDPM(**kwargs)\n",
        "    model.train()\n",
        "    trainer.fit(model, train_dataloader, test_dataloader)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "model = train_model(T=1000).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQU8IOMAZFhf"
      },
      "outputs": [],
      "source": [
        "pl.seed_everything(1)\n",
        "sampled_data = model.sample(16)[:, -1]               # Get the last time step for each image\n",
        "show_imgs(sampled_data, nrows=4, grid=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URB1vjLsbCW0"
      },
      "source": [
        "## Autocomplete two images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1YhKj8MbCW1"
      },
      "outputs": [],
      "source": [
        "def complete_image(y=None, ymask=None, scale=1.):\n",
        "    # Implement here\n",
        "    # Hint: Use ddpm.get_x0_pred(x_t, t, e_t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL2uzCGEY-x0"
      },
      "source": [
        "# Bonus - Adding context\n",
        "<font color='red'>If you did the bonus, write it here</font><br/>\n",
        "\n",
        "MNIST is boring! Moreover, what is the point of generating samples without controling them? <br/>\n",
        "We will use a pretrained CLIP (Contrastive Language-Image Pre-Training). Given text, creates embedding. Our goal is to align image description and text embedding to each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaaUMZPat0pd"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet git+https://github.com/openai/CLIP.git\n",
        "\n",
        "import glob\n",
        "import csv\n",
        "from textwrap import wrap\n",
        "\n",
        "import clip\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDQzWRRMlLYT"
      },
      "source": [
        "## Load data\n",
        "Go to your <a href=\"https://www.kaggle.com/\">Kaggle</a> account and under the settings, generate new API token. <br/>\n",
        "This will export you a json file, which you will upload here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsQP__wzjHQp"
      },
      "outputs": [],
      "source": [
        "# The script expects you to upload JSON file to it!\n",
        "\n",
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets list\n",
        "! kaggle datasets download jessicali9530/celeba-dataset\n",
        "! unzip -q celeba-dataset.zip -d faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRoYygVmk2iU"
      },
      "outputs": [],
      "source": [
        "FACES_PATH = \"/content/faces/img_align_celeba/img_align_celeba\"\n",
        "\n",
        "for i in range(1,9):\n",
        "  img = Image.open(f'{FACES_PATH}/00000{i}.jpg')\n",
        "  plt.subplot(2, 4, i)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(img)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqU0DIjst5a7"
      },
      "source": [
        "## load pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5FzjTmiT42i"
      },
      "outputs": [],
      "source": [
        "clip.available_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8zRXdOAt9wH"
      },
      "outputs": [],
      "source": [
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\")\n",
        "clip_model.eval()\n",
        "CLIP_FEATURES = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvgB41A-naVP"
      },
      "source": [
        "## Intro to CLIP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_0bO2pR3OEO"
      },
      "source": [
        "Load image using CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTSyqKPpQSPq"
      },
      "outputs": [],
      "source": [
        "img = Image.open(f'{FACES_PATH}/000001.jpg')\n",
        "\n",
        "clip_imgs = torch.tensor(np.stack([clip_preprocess(img)])).to(device)\n",
        "print(\"After image clip preprocessing the size is \", clip_imgs.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqhqiEt530ml"
      },
      "source": [
        "Feature extractor of CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jNKDvgm32oS"
      },
      "outputs": [],
      "source": [
        "clip_img_encoding = clip_model.encode_image(clip_imgs)\n",
        "print(clip_img_encoding.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igam8MX75YwZ"
      },
      "source": [
        "Now, we want to see how to tokenize text and encoder it using clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KR_FILDZ5dFl"
      },
      "outputs": [],
      "source": [
        "text_list = [\n",
        "    \"An Angry man\",\n",
        "    \"Smiling bald person\",\n",
        "    \"Happy beautiful woman\"\n",
        "]\n",
        "text_tokens = clip.tokenize(text_list).to(device)\n",
        "print(\"Text tokens\")\n",
        "print(text_tokens[:,:10])\n",
        "print(\"----------------------------\")\n",
        "\n",
        "clip_text_encodings = clip_model.encode_text(text_tokens).float()\n",
        "print(\"For each text, encoding of 512 features \", clip_text_encodings.size())\n",
        "print(clip_text_encodings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgr5aa1b6kEy"
      },
      "source": [
        "In order to see which one of our text descriptions best describes the daisy, we can calculate the cosine similarity between the text encodings and the image encodings. <br/>\n",
        "We will load three flowers, give each its encoding and will compare to the texts above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykxsTFug7U2d"
      },
      "outputs": [],
      "source": [
        "def get_img_encodings(imgs):\n",
        "    processed_imgs = [clip_preprocess(img) for img in imgs]\n",
        "    clip_imgs = torch.tensor(np.stack(processed_imgs)).to(device)\n",
        "    clip_img_encodings = clip_model.encode_image(clip_imgs)\n",
        "    return clip_img_encodings\n",
        "\n",
        "imgs = [Image.open(f\"{FACES_PATH}/{i}.jpg\") for i in [\"000069\", \"000174\", \"000154\"]]\n",
        "for i, img in enumerate(imgs):\n",
        "    plt.subplot(1,3,i+1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f3bp5rW7xQU"
      },
      "outputs": [],
      "source": [
        "clip_img_encodings = get_img_encodings(imgs)    # torch.Tensor([3, 512])\n",
        "\n",
        "text_list = [\n",
        "    \"A surprised man with black hair\",\n",
        "    \"A woman smiling with red hair\",\n",
        "    \"A person with black glasses and wears black hat\"\n",
        "]\n",
        "\n",
        "text_tokens = clip.tokenize(text_list).to(device)\n",
        "clip_text_encodings = clip_model.encode_text(text_tokens)   # torch.Tensor([3, 512])\n",
        "\n",
        "clip_img_encodings /= clip_img_encodings.norm(dim=-1, keepdim=True)\n",
        "clip_text_encodings /= clip_text_encodings.norm(dim=-1, keepdim=True)\n",
        "\n",
        "similarity = clip_img_encodings @ clip_text_encodings.T\n",
        "\n",
        "print(similarity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-dFXoeX9aCR"
      },
      "source": [
        "Well, is there a match?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDcUIzbW87MA"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10, 10))\n",
        "gs = fig.add_gridspec(2, 3, wspace=.1, hspace=0)\n",
        "\n",
        "ax = fig.add_subplot(gs[1, :])\n",
        "plt.imshow(similarity.detach().cpu().numpy().T, vmin=0.1, vmax=0.3)\n",
        "\n",
        "labels = [ '\\n'.join(wrap(text, 20)) for text in text_list ]\n",
        "plt.yticks(range(len(text_tokens)), labels, fontsize=10)\n",
        "plt.xticks([])\n",
        "\n",
        "for x in range(similarity.shape[1]):\n",
        "    for y in range(similarity.shape[0]):\n",
        "        plt.text(x, y, f\"{similarity[x, y]:.2f}\", ha=\"center\", va=\"center\", size=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfVr3jwTdBVG"
      },
      "source": [
        "Collabory: CLIP gives the most similar encoding of image to the most similar encoding of text.\n",
        "Hence, we will train using the image encoding, but create new images using an encoding of text, hopefully it will work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaVgCZGFwuss"
      },
      "source": [
        "## Proccess the data using clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SMJJVCixBY-"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 32\n",
        "IMG_CH = 3\n",
        "BATCH_SIZE = 128\n",
        "CLIP_FEATURES = 512\n",
        "\n",
        "def crop_face(sample):\n",
        "  return sample[:, 9:(9+32),4:(4+32)]\n",
        "\n",
        "pre_transforms = transforms.Compose([\n",
        "    transforms.Resize((50, 40)),\n",
        "    transforms.ToTensor(),  # Scales data into [0,1]\n",
        "    crop_face,\n",
        "    transforms.Lambda(lambda t: (t * 2) - 1)  # Scale between [-1, 1]\n",
        "])\n",
        "\n",
        "random_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X268JiRQxMft"
      },
      "source": [
        "With our current resources, we DO NOT want to encode ~60000 imaegs. Download the csv file from the task pdf. It contains the file paths, along with its preprocessed CLIP data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1l_E-mIxKcq"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import csv\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, csv_path='clip_data.csv'):\n",
        "        self.imgs = []\n",
        "        self.labels = torch.empty(\n",
        "            len(data_paths), CLIP_FEATURES, dtype=torch.float, device=device\n",
        "        )\n",
        "\n",
        "        with open(csv_path, newline='') as csvfile:\n",
        "            reader = csv.reader(csvfile, delimiter=',')\n",
        "            for idx, row in enumerate(reader):\n",
        "                img = Image.open(row[0])\n",
        "                self.imgs.append(pre_transforms(img).to(device))\n",
        "                label = [float(x) for x in row[1:]]\n",
        "                self.labels[idx, :] = torch.FloatTensor(label).to(device)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return random_transforms(self.imgs[idx]), self.labels[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "data_paths = glob.glob(f'{FACES_PATH}/*.jpg', recursive=True)\n",
        "train_data = MyDataset(csv_path='/content/drive/MyDrive/clip.csv')\n",
        "\n",
        "faces_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3cmjqOhxcbz"
      },
      "source": [
        "## Modify the architercture to bring context into our model\n",
        "\n",
        "<img width=\"70%\" src=\"https://sharon.srworkspace.com/dgm/context1.png\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sO4FUfVnxcbz"
      },
      "outputs": [],
      "source": [
        "#@title Components of UNet\n",
        "import math\n",
        "\n",
        "class GELUConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, group_size):\n",
        "        super().__init__()\n",
        "        layers = [\n",
        "            nn.Conv2d(in_ch, out_ch, 3, 1, 1),\n",
        "            nn.GroupNorm(group_size, out_ch),\n",
        "            nn.GELU(),\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class RearrangePoolBlock(nn.Module):\n",
        "    def __init__(self, in_chs, group_size):\n",
        "        super().__init__()\n",
        "        self.rearrange = nn.MaxPool2d(2)\n",
        "        self.conv = GELUConvBlock(in_chs, in_chs, group_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rearrange(x)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class DownBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super(DownBlock, self).__init__()\n",
        "        layers = [\n",
        "            GELUConvBlock(in_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            RearrangePoolBlock(out_chs, group_size),\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super(UpBlock, self).__init__()\n",
        "        layers = [\n",
        "            nn.ConvTranspose2d(2 * in_chs, out_chs, 2, 2),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x = torch.cat((x, skip), 1)\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SinusoidalPositionEmbedBlock(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class EmbedBlock(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim):\n",
        "        super(EmbedBlock, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        layers = [\n",
        "            nn.Linear(input_dim, emb_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(emb_dim, emb_dim),\n",
        "            nn.Unflatten(1, (emb_dim, 1, 1)),\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.input_dim)\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class ResidualConvBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super().__init__()\n",
        "        self.conv1 = GELUConvBlock(in_chs, out_chs, group_size)\n",
        "        self.conv2 = GELUConvBlock(out_chs, out_chs, group_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv1(x)\n",
        "        x2 = self.conv2(x1)\n",
        "        out = x1 + x2\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTutNzEGxcb0"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, T):\n",
        "        super(UNet, self).__init__()\n",
        "        img_chs = IMG_CH\n",
        "        self.T = T\n",
        "        down_chs = (32, 64, 128)\n",
        "        up_chs = down_chs[::-1]  # Reverse of the down channels\n",
        "        latent_image_size = IMG_SIZE // 4 # 2 ** (len(down_chs) - 1)\n",
        "        t_embed_dim = 16\n",
        "        c_embed_dim=CLIP_FEATURES # New\n",
        "\n",
        "        small_group_size = 8 # New\n",
        "        big_group_size = 32 # New\n",
        "\n",
        "        # Inital convolution\n",
        "        self.down0 = ResidualConvBlock(IMG_CH, down_chs[0], small_group_size)\n",
        "\n",
        "        # Downsample\n",
        "        self.down1 = DownBlock(down_chs[0], down_chs[1], big_group_size)\n",
        "        self.down2 = DownBlock(down_chs[1], down_chs[2], big_group_size)\n",
        "        self.to_vec = nn.Sequential(nn.Flatten(), nn.GELU())\n",
        "\n",
        "        # Embeddings\n",
        "        self.dense_emb = nn.Sequential(\n",
        "            nn.Linear(down_chs[2] * latent_image_size**2, down_chs[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(down_chs[1], down_chs[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(down_chs[1], down_chs[2] * latent_image_size**2),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.sinusoidaltime = SinusoidalPositionEmbedBlock(t_embed_dim)\n",
        "        self.temb_1 = EmbedBlock(t_embed_dim, up_chs[0])\n",
        "        self.temb_2 = EmbedBlock(t_embed_dim, up_chs[1])\n",
        "        self.c_embed1 = EmbedBlock(c_embed_dim, up_chs[0])\n",
        "        self.c_embed2 = EmbedBlock(c_embed_dim, up_chs[1])\n",
        "\n",
        "        # Upsample\n",
        "        self.up0 = nn.Sequential(\n",
        "            nn.Unflatten(1, (up_chs[0], latent_image_size, latent_image_size)),\n",
        "            GELUConvBlock(up_chs[0], up_chs[0], big_group_size),\n",
        "        )\n",
        "        self.up1 = UpBlock(up_chs[0], up_chs[1], big_group_size)\n",
        "        self.up2 = UpBlock(up_chs[1], up_chs[2], big_group_size)\n",
        "\n",
        "        # Match output channels and one last concatenation\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Conv2d(2 * up_chs[-1], up_chs[-1], 3, 1, 1),\n",
        "            nn.GroupNorm(small_group_size, up_chs[-1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(up_chs[-1], IMG_CH, 3, 1, 1),\n",
        "        )\n",
        "\n",
        "        print(\"Num params: \", sum(p.numel() for p in self.parameters()))\n",
        "\n",
        "\n",
        "    def forward(self, x, t, c, c_mask):\n",
        "        # Implement here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbnTlwrNyFcR"
      },
      "outputs": [],
      "source": [
        "class DDPM(pl.LightningModule):\n",
        "    def __init__(self, T, method='cosine'):\n",
        "        super(DDPM, self).__init__()\n",
        "        self.T = T\n",
        "\n",
        "        epsilon=0.008\n",
        "        if method == 'cosine':\n",
        "            steps=torch.linspace(0,T,steps=T+1).to(device)\n",
        "            f_t=torch.cos(((steps/T+epsilon)/(1.0+epsilon))*math.pi*0.5)**2\n",
        "            self.Beta = torch.clip(1.0-f_t[1:]/f_t[:T], 0.0, 0.999)\n",
        "\n",
        "        elif method == 'linear':\n",
        "            self.Beta = torch.linspace(1e-4, 2e-2, T).to(device)\n",
        "\n",
        "        # Forward diffusion variables\n",
        "        self.a = 1.0 - self.Beta\n",
        "        self.a_bar = torch.cumprod(self.a, dim=0)\n",
        "\n",
        "        self.net = UNet(T)\n",
        "\n",
        "        # Logging\n",
        "        self.train_loss = []\n",
        "        self.train_loss_in_epoch = []\n",
        "\n",
        "    def q(self, x_0, t):\n",
        "        \"\"\"\n",
        "        Samples a new image from q\n",
        "        Returns the noise applied to an image at timestep t\n",
        "        x_0: the original image\n",
        "        t: timestep\n",
        "        \"\"\"\n",
        "        # Implement here\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def reverse_q(self, x_t, t, e_t):\n",
        "        # Implement here\n",
        "\n",
        "    def get_context_mask(self, c, drop_prob=0.1):\n",
        "        c_mask = torch.bernoulli(torch.ones_like(c).float() - drop_prob).to(device)\n",
        "        return c_mask\n",
        "\n",
        "    def get_loss(self, x_0, t, c):\n",
        "        \"\"\"\n",
        "        Returns the loss between the true noise and the predicted noise\n",
        "        x_0: the original image\n",
        "        t: timestep\n",
        "        \"\"\"\n",
        "        # Implement here\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, text_list, s=0.5):\n",
        "        # Implement here\n",
        "        return x_t      # (|text_list|, IMG_CH, IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "\n",
        "    # Lightning Configurations\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.99)\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, c = batch\n",
        "        t = torch.randint(0, self.T, (BATCH_SIZE,), device=device)\n",
        "\n",
        "        loss = self.get_loss(x, t, c)\n",
        "        self.train_loss_in_epoch.append(loss.item())\n",
        "        return loss\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        avg_loss = torch.tensor(self.train_loss_in_epoch).mean()\n",
        "        self.train_loss_in_epoch = []\n",
        "\n",
        "        self.log(\"train_loss\", avg_loss, prog_bar=True)\n",
        "        self.train_loss.append(avg_loss.detach().item())\n",
        "\n",
        "        print(f\"Epoch {self.current_epoch} | Loss: {avg_loss.detach().item()} \")\n",
        "        sampled_data = self.sample(text_list, s=1.)\n",
        "        show_imgs(sampled_data, grid=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo04CKG6NoSe"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idaf78ZGc_X0"
      },
      "source": [
        "This text list will be sampled at the end of each epoch, but does not affect the train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OC42EEI9yhLZ"
      },
      "outputs": [],
      "source": [
        "# Change me\n",
        "text_list = [\n",
        "    \"A man wearing a white hat\",\n",
        "    \"A woman in sun glasses\",\n",
        "    \"A man with green hair and a blue shirt\",\n",
        "    \"A sad woman with blue eyes\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29xs-z05P61u"
      },
      "outputs": [],
      "source": [
        "def train_model(**kwargs):\n",
        "    # Create a PyTorch Lightning trainer with the generation callback\n",
        "    trainer = pl.Trainer(\n",
        "        default_root_dir=os.path.join(\"DDPM\"),\n",
        "        devices=1,\n",
        "        max_epochs=50,\n",
        "        callbacks=[\n",
        "            ModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"train_loss\"),\n",
        "            LearningRateMonitor(\"epoch\")\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model = DDPM(**kwargs)\n",
        "    model.train()\n",
        "    trainer.fit(model, faces_dataloader)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "model = train_model(T=300, method='linear').to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkZDrwm0QEfb"
      },
      "source": [
        "## Final evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lR-7xU5RQENk"
      },
      "outputs": [],
      "source": [
        "text_list_new = [\n",
        "    \"A sad man with long hair\",\n",
        "    \"A smiling woman with green eyes\",\n",
        "]\n",
        "\n",
        "sampled_data = model.sample(text_list_new, s=0)\n",
        "show_imgs(sampled_data, grid=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "1MWoe14Ai3I-",
        "HA5GrP2zY9bI",
        "renhtvkaZBGA",
        "c5ErXXVQqSye",
        "MVdI8FjrZgRx",
        "wOh7AyR5Zd8Z",
        "URB1vjLsbCW0",
        "IL2uzCGEY-x0",
        "EDQzWRRMlLYT",
        "fqU0DIjst5a7",
        "qvgB41A-naVP",
        "HaVgCZGFwuss",
        "n3cmjqOhxcbz",
        "Zo04CKG6NoSe",
        "EkZDrwm0QEfb"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
