{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MWoe14Ai3I-"
      },
      "source": [
        "# Exercise 7: Denoising Diffusion Probabilistic Models\n",
        "\n",
        "Submitted by:\n",
        "\n",
        " ** Faisal Omari 325616894\n",
        "\n",
        " ** Fadi Khatib 308052992"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA5GrP2zY9bI"
      },
      "source": [
        "### Load libraries and utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "lzI4F5xRY9bK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Seed set to 42\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device cuda:0\n"
          ]
        }
      ],
      "source": [
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import os\n",
        "\n",
        "# Visualization tools\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip install --quiet pytorch-lightning>=1.4\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
        "print(\"Using device\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "VEjj8YMDY9bL"
      },
      "outputs": [],
      "source": [
        "def show_tensor_image(img):\n",
        "    reverse_transforms = transforms.Compose([\n",
        "        transforms.Lambda(lambda t: (t + 1) / 2),\n",
        "        transforms.Lambda(lambda t: torch.minimum(torch.tensor([1]), t)),\n",
        "        transforms.Lambda(lambda t: torch.maximum(torch.tensor([0]), t)),\n",
        "        transforms.ToPILImage(),\n",
        "    ])\n",
        "    plt.imshow(reverse_transforms(img))\n",
        "\n",
        "def trim_imgs(imgs, skip=10):\n",
        "    imgs = imgs.view((-1,) + imgs.shape[2:])        # BATCH * T, CH, WIDTH, HEIGHT\n",
        "    imgs = imgs[::skip]\n",
        "    return imgs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "renhtvkaZBGA"
      },
      "source": [
        "### Load MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "_KOpH4ZKjPIC"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 28\n",
        "IMG_CH = 1\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "def load_MNIST(data_transform, train=True):\n",
        "    return torchvision.datasets.MNIST(\n",
        "        \"./\",\n",
        "        download=True,\n",
        "        train=train,\n",
        "        transform=data_transform,\n",
        "    )\n",
        "\n",
        "\n",
        "def load_transformed_MNIST():\n",
        "    data_transforms = [\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor(),  # Scales data into [0,1]\n",
        "        transforms.Lambda(lambda t: (t * 2) - 1)  # Scale between [-1, 1]\n",
        "    ]\n",
        "\n",
        "    data_transform = transforms.Compose(data_transforms)\n",
        "    train_set = load_MNIST(data_transform, train=True)\n",
        "    test_set = load_MNIST(data_transform, train=False)\n",
        "    return train_set, test_set\n",
        "\n",
        "train, test = load_transformed_MNIST()\n",
        "train_dataloader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "test_dataloader = DataLoader(test, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "O6sLfpUfZBGL"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def show_imgs(imgs, nrows=1, grid=True):\n",
        "    ncols = len(imgs) // nrows\n",
        "\n",
        "    imgs = torch.stack(imgs) if isinstance(imgs, list) else imgs\n",
        "    imgs = imgs.unsqueeze(1) if imgs.dim() == 3 else imgs\n",
        "\n",
        "    if grid:\n",
        "        grid = torchvision.utils.make_grid(imgs.cpu(), nrow=ncols, pad_value=128)\n",
        "        show_tensor_image(grid.detach().cpu())\n",
        "    else:\n",
        "        ncols = len(imgs) // nrows\n",
        "        for idx, img in enumerate(imgs):\n",
        "            plt.subplot(nrows, ncols, idx + 1)\n",
        "            plt.axis('off')\n",
        "            show_tensor_image(img.detach().cpu())\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "show_imgs([train[i][0] for i in range(8)], nrows=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5ErXXVQqSye"
      },
      "source": [
        "## Define the architercture\n",
        "\n",
        "<img width=\"70%\" src=\"https://sharon.srworkspace.com/dgm/time.png\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "cellView": "form",
        "id": "hKrRnXWfrfOu"
      },
      "outputs": [],
      "source": [
        "#@title Net components\n",
        "\n",
        "class EmbedBlock(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim):\n",
        "        super(EmbedBlock, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        layers = [\n",
        "            nn.Linear(input_dim, emb_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(emb_dim, emb_dim),\n",
        "            nn.Unflatten(1, (emb_dim, 1, 1)),\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.input_dim)\n",
        "        #x = x[:, None]\n",
        "        return self.model(x)\n",
        "\n",
        "class SinusoidalPositionEmbedBlock(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings\n",
        "\n",
        "class DownBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(in_chs, out_chs, 3, 1, 1),\n",
        "            nn.BatchNorm2d(out_chs),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_chs, out_chs, 3, 1, 1),\n",
        "            nn.BatchNorm2d(out_chs),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.ConvTranspose2d(2 * in_chs, out_chs, 3, 2, 1, 1),\n",
        "            nn.BatchNorm2d(out_chs),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_chs, out_chs, 3, 1, 1),\n",
        "            nn.BatchNorm2d(out_chs),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x = torch.cat((x, skip), 1)\n",
        "        x = self.model(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "wv8_SKFBsaST"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, T):\n",
        "        super(UNet, self).__init__()\n",
        "        self.T = T\n",
        "\n",
        "        img_chs = IMG_CH\n",
        "        down_chs = (64, 128, 128)\n",
        "        up_chs = down_chs[::-1]  # Reverse of the down channels\n",
        "        self.latent_image_size = IMG_SIZE // 4 # Store this as an instance variable\n",
        "\n",
        "        # New\n",
        "        t_embed_dim = 8\n",
        "\n",
        "        # Initial convolution\n",
        "        self.down0 = nn.Sequential(\n",
        "            nn.Conv2d(img_chs, down_chs[0], 3, padding=1),\n",
        "            nn.BatchNorm2d(down_chs[0]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Downsample\n",
        "        self.down1 = DownBlock(down_chs[0], down_chs[1])\n",
        "        self.down2 = DownBlock(down_chs[1], down_chs[2])\n",
        "        self.to_vec = nn.Sequential(nn.Flatten(), nn.ReLU())\n",
        "\n",
        "        # Embeddings\n",
        "        self.dense_emb = nn.Sequential(\n",
        "            nn.Linear(down_chs[2] * self.latent_image_size**2, down_chs[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(down_chs[1], down_chs[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(down_chs[1], down_chs[2] * self.latent_image_size**2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.sinusoidaltime = SinusoidalPositionEmbedBlock(t_embed_dim) # New\n",
        "        self.temb_1 = EmbedBlock(t_embed_dim, up_chs[0]) # New\n",
        "        self.temb_2 = EmbedBlock(t_embed_dim, up_chs[1]) # New\n",
        "\n",
        "        # Upsample\n",
        "        self.up0 = nn.Sequential(\n",
        "            nn.Unflatten(1, (up_chs[0], self.latent_image_size, self.latent_image_size)),\n",
        "            nn.Conv2d(up_chs[0], up_chs[0], 3, padding=1),\n",
        "            nn.BatchNorm2d(up_chs[0]),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.up1 = UpBlock(up_chs[0], up_chs[1])\n",
        "        self.up2 = UpBlock(up_chs[1], up_chs[2])\n",
        "\n",
        "        # Match output channels\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Conv2d(up_chs[2] + down_chs[0], up_chs[2], 3, 1, 1),\n",
        "            nn.BatchNorm2d(up_chs[-1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(up_chs[-1], img_chs, 3, 1, 1)\n",
        "        )\n",
        "\n",
        "        print(\"Net Num params: \", sum(p.numel() for p in self.parameters()))\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        # Initial downsampling\n",
        "        down0 = self.down0(x)\n",
        "        down1 = self.down1(down0)\n",
        "        down2 = self.down2(down1)\n",
        "        latent_vec = self.to_vec(down2)\n",
        "        \n",
        "        # Embed the timestep\n",
        "        t = t.float() / self.T\n",
        "        t_emb = self.sinusoidaltime(t)\n",
        "        \n",
        "        latent_vec = self.dense_emb(latent_vec)\n",
        "        temb_1 = self.temb_1(t_emb)\n",
        "        temb_2 = self.temb_2(t_emb)\n",
        "        \n",
        "        # Upsampling\n",
        "        up0 = self.up0(latent_vec.view(latent_vec.size(0), -1, self.latent_image_size, self.latent_image_size))\n",
        "        up1 = self.up1(up0 + temb_1[:, :, None, None], down2)\n",
        "        up2 = self.up2(up1 + temb_2[:, :, None, None], down1)\n",
        "        \n",
        "        # Concatenate the initial downsampling output for the final layer\n",
        "        return self.out(torch.cat((up2, down0), dim=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVdI8FjrZgRx"
      },
      "source": [
        "## Define DDPM model\n",
        "\n",
        "A fundemantal idea of diffusion models is to add a little noise to the image each time step and learn how to remove it, depending on time. Here, we will use variance schedule.\n",
        "\n",
        "<img width=\"70%\" src=\"https://sharon.srworkspace.com/dgm/dog.png\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "caQug2W_ZgRz"
      },
      "outputs": [],
      "source": [
        "class DDPM(pl.LightningModule):\n",
        "    def __init__(self, T=1000, method='cosine'):\n",
        "        super(DDPM, self).__init__()\n",
        "        self.T = T\n",
        "\n",
        "        epsilon=0.008\n",
        "        if method == 'cosine':\n",
        "            steps=torch.linspace(0,T,steps=T+1).to(device)\n",
        "            f_t=torch.cos(((steps/T+epsilon)/(1.0+epsilon))*math.pi*0.5)**2\n",
        "            self.Beta = torch.clip(1.0-f_t[1:]/f_t[:T], 0.0, 0.999)\n",
        "\n",
        "        elif method == 'linear':\n",
        "            self.Beta = torch.linspace(1e-4, 2e-2, T).to(device)\n",
        "\n",
        "        # Forward diffusion variables\n",
        "        self.a = 1.0 - self.Beta\n",
        "        self.a_bar = torch.cumprod(self.a, dim=0)\n",
        "\n",
        "        self.net = UNet(T)\n",
        "\n",
        "        # Logging\n",
        "        self.train_loss = []\n",
        "        self.train_loss_in_epoch = []\n",
        "\n",
        "        self.validation_loss = []\n",
        "        self.validation_loss_in_epoch = []\n",
        "\n",
        "    def q(self, x_0, t):\n",
        "        \"\"\"\n",
        "        Samples a new image from q\n",
        "        Returns the noise applied to an image at timestep t\n",
        "        x_0: the original image\n",
        "        t: timestep\n",
        "        \"\"\"\n",
        "        # Implement here\n",
        "        noise = torch.randn_like(x_0).to(x_0.device)\n",
        "        mean = torch.sqrt(self.a_bar[t])[:, None, None, None] * x_0\n",
        "        std = torch.sqrt(1 - self.a_bar[t])[:, None, None, None] * noise\n",
        "        return mean + std, noise\n",
        "\n",
        "    # To be used later\n",
        "    def get_x0_pred(self, x_t, t, e_t):\n",
        "        x_0_pred = (x_t - torch.sqrt(1 - self.a_bar[t]) * e_t) / torch.sqrt(self.a_bar[t])\n",
        "        x_0_pred.clamp_(-1, 1)\n",
        "        return x_0_pred\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def reverse_q(self, x_t, t, e_t):\n",
        "        # Implement here\n",
        "        coeff = (self.Beta[t] / torch.sqrt(1 - self.a_bar[t]))[:, None, None, None]\n",
        "        mean = (x_t - coeff * e_t) / torch.sqrt(self.a[t])[:, None, None, None]\n",
        "        std = torch.sqrt(self.Beta[t])[:, None, None, None]\n",
        "        return mean + std * torch.randn_like(x_t).to(x_t.device)\n",
        "\n",
        "    def get_loss(self, x_0, t):\n",
        "        \"\"\"\n",
        "        Returns the loss between the true noise and the predicted noise\n",
        "        x_0: the original image\n",
        "        t: timestep\n",
        "        \"\"\"\n",
        "        # Implement here\n",
        "        noise = torch.randn_like(x_0).to(x_0.device)\n",
        "        x_t, _ = self.q(x_0, t)\n",
        "        e_t = self.net(x_t, t)\n",
        "        return F.mse_loss(noise, e_t)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, num_imgs=1):\n",
        "        output = torch.zeros((num_imgs, self.T, IMG_CH, IMG_SIZE, IMG_SIZE))  # For each image, save all the timesteps\n",
        "        x_t = torch.randn((num_imgs, IMG_CH, IMG_SIZE, IMG_SIZE)).to(device)\n",
        "        for t in reversed(range(self.T)):\n",
        "            e_t = self.net(x_t, torch.tensor([t] * num_imgs, device=device))\n",
        "            x_t = self.reverse_q(x_t, torch.tensor([t] * num_imgs, device=device), e_t)\n",
        "            output[:, t] = x_t\n",
        "        return output\n",
        "\n",
        "    # Lightning Configurations\n",
        "    def configure_optimizers(self):\n",
        "        return Adam(self.parameters(), lr=1e-3)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x = batch[0].to(device)\n",
        "        t = torch.randint(0, self.T, (BATCH_SIZE,), device=device)\n",
        "\n",
        "        loss = self.get_loss(x, t)\n",
        "        self.train_loss_in_epoch.append(loss.item())\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x = batch[0].to(device)\n",
        "        t = torch.randint(0, self.T, (BATCH_SIZE,), device=device)\n",
        "\n",
        "        loss = self.get_loss(x, t)\n",
        "        self.validation_loss_in_epoch.append(loss.item())\n",
        "        return loss\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        avg_loss = torch.tensor(self.train_loss_in_epoch).mean()\n",
        "        self.train_loss_in_epoch = []\n",
        "\n",
        "        self.log(\"train_loss\", avg_loss, prog_bar=True)\n",
        "        self.train_loss.append(avg_loss.detach().item())\n",
        "\n",
        "        print(f\"Epoch {self.current_epoch} | Loss: {avg_loss.detach().item()} \")\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        avg_loss = torch.tensor(self.validation_loss_in_epoch).mean()\n",
        "        self.validation_loss_in_epoch = []\n",
        "\n",
        "        self.log(\"val_loss\", avg_loss, prog_bar=True)\n",
        "        self.validation_loss.append(avg_loss.detach().item())\n",
        "\n",
        "        sampled_data = trim_imgs(self.sample(), skip=100)\n",
        "        show_imgs(sampled_data, grid=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "cellView": "form",
        "id": "UC3z8ECxZgR0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Net Num params:  3032449\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABFAAAABvCAYAAADL5DRXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB4oklEQVR4nO29Z3hUZdf+fc1MQnojCWkQ0ukQpIgUQXoRRASkqShSpIkFsPdekCKIFFEUFERBpQsI0nvokE4CIZAEUkjPzLwfnuPe51rbmQRwuJ/n/x7r92nt7Ovas/e+6kzWuZbBarValSAIgiAIgiAIgiAIgmAX4//2DQiCIAiCIAiCIAiCIPxfR35AEQRBEARBEARBEARBqAH5AUUQBEEQBEEQBEEQBKEG5AcUQRAEQRAEQRAEQRCEGpAfUARBEARBEARBEARBEGpAfkARBEEQBEEQBEEQBEGoAfkBRRAEQRAEQRAEQRAEoQbkBxRBEARBEARBEARBEIQacLrVgj2MQ3BgMPCTVquj7ufu4oj7pteorv6tlKvmfv60/Hz792aDPvWe1eyqy1nsnCmojmabr16zWd/YtCE7tl5IhV1ZcUv3YPTw0GxLcXE1BU2wLeZburbdS7m7s2NLSQnOeXlptiE8lFfMwnuwlpaiflmZZjuF8TpWbzzf5jMf3NkN26B3k1c0uzzUm51z2n7UYZ9THbfSR6qjoldrduy6+6xmG/1ra3Z2n3qa7ZPK+5Vbco5mV6Vn3NLnWjq31Gzny/mabU5Os1vHUWOu7eOfa7bPDwfYOaOrK+zQYM3O7h6i2b4p/PnttnXbZvz40CnNTFkZr9n1ljtrdq3Nh1mVa5Paa3boJjI/VFaxckWt0OedizA2b8TW0uw6hwtZHWMG+sulx2I1O2TWPmWPxK/aanbMT5W41q7jdus4qt3iJ83SbO8M/vwuGw7ri9dI2kf3aXbkS/s1u+4BT1buUrubmk3nJktRkWaXPHwvq1Py1A3NDuifqNkmXx9WzpxfUON93njiPnZsHpyn2XUevaTZGd9HsnL+yzHvuf12SLMNLZtotjXhLKtzN9Y4pZSK+gJjzjWH/1+owhefSduBUt16odo1h33gJCuX83sDzS5M9NPs6Bcx7m8O4W3X8IUzmn3ui6aaXfV4Hivn1y/J5r1mP4cxW8m7kgpfj/YuaIi+dDOMv5PQzzAGUz5vh/t+Afdt7RDP6pQFYKzvWTvd5r3dLpZszAstDw9j5+o8dF6zM1/HM5fWJ/Ojme+h4sbbHqfmLvew4yv3YR4O3U32BnsSNNvUIIZf40KyZjc/hs89eY/9faC98Zw3ho85j2uYUz2PYI079zHfa0Qvxmc5FZD9yUm8K/1cUemOez30/Qt27/V26NYZ+5w/Vy1j53qFxtdYf0tWAjvu06CTZtP3lLioDSsXN460LxmXTpf42KFYfNEGqa+jDx9ov5CVG1YPfYzu8ei+OWlOO1bH4F+u2f5/ok+1nXyMlUtqU65sUd4Pz+eefIOdo/3NkXPle6cf1Oxv/nyAnaPj//fLeNfzb2Ce29KU70VpW3Z9fIxm5zVzYeWCv8CckzcW/d/S/7pmB3zkyuooMrQye2G9qfC2sGINPsb3kqyhGLdB8/CZ+j7Xefw4zb48HHNKcpdvWTl7/blwOPrC/s95X6J1HNV2KZnYI05q1pedi9qO/kX7Gt0PVLcXMD+A+XHbim/YucEp3TV7TfQ2zW56YKRm67/ium1GH6lyw/wTeKyElauVkq3ZG45u1uyonydoduyzfA8dfhD9IONefJ+89EsTVs58Bvdw/umvNLtvN/yWkTQ6gNUxkG6VPPN5VRPigSIIgiAIgiAIgiAIglAD8gOKIAiCIAiCIAiCIAhCDdyyhIfxb6UvjrjGndR3hNToVq9xK+X+C9InvWyHYnCDu5yhFXG7PgoXY2Xi7WZPtmNw4e561gqUM9aGazOT8FDXaKWUOnhK2cKpXl12XJV5yWY5KgGqaNeIX2MHZBDURVSducDKmfxwr1RiYiHv0erJ3b3N52y7Wv9brCY8T3WSHfrureW2XUVvB/q+7b7rajDGN9bsWluOsHPU8ZK6yQcswufo3emrSDnqVm7Ym2D/Hojkg4rBLJ1a8nK77UtD7hQq2zE1jmPnEsegT8Uuy9fsgEWQFlD5UbUc4uPF2Bxyu7h3Mc7MXjq3WELQPrh1VqWm41pEdqeUUk43g2CTvhi4nXx+bBSrY46ERMkjm7vcsmuHoJx7BpYk4y7IQjJfa8/q1HvPvgzoTinsBJf4wLn8ftPfhctxxOtoq2sTcV9Bh4tYnYDjtud3KtlRird32jPorZHDIBdxX3uQ1TFY2ipbGHQSnsvjMa/XX31Zs692h4u6/2Iua8moj2eylEAeVJrN9SI34jA/FUxDneDZaBv9WL40Od7mff9bIn/HvOecw2WimX3hpps7Du1Ix9z5L7j7b+MPr2p2lU62Q3H73lez6/yOueTCArSPSy7/P9WhX7HuhaYRucJVnfxqPO418BDkcV4Z6CNdXufjYJVXR82OmkHadTiXHlCo237GW2jH8Lf4td3sXuHO6TlktGY7vczdu6lcwqkEYymt7xLNbvnBRFan00lIbXc3x7x3aWIlK+eyj0gpiWyHUhnI+/vF0WiPoW5rNfukCmbl2BpD1peSQZDW+C+1LSVTSqmcDZA1xfbjUpDiR3CNa33wDJGki+rnCsb39k/dDrUyIZmJ3v4kO+fzDO4r8Cs8Z+ISyHl7D2jK6hhNmTY/J+onPg9XdWul2XQdKu2Ovztv43ul6LW4H0sb9I/hzl10n0ZkHBPqa3aLBzC3GHWvNnoU2vcGabe0AX68oIJUgUsQIJPRi9aT5t6r7gavBGDP++ef97NzKZ9hzPUbBVlH6sOQAccq/hIi/xir2WnLF2u2XvpyZR325GHD8N4sSzE2TbFcJvru5pWanVAWrtkfbBnIyhV1QL3Q7/H9xUr2ove+xGVzB7+GrGNrCZ6vT08uJVQK8rjk2Xg/MdMwb773Mg91oN8/OYKBs2ZodnAhn5u/DEvQ7F4qXrOzh+H5j73xlaK0mwGZzIFPIEFqlzCYlbua5avZD7yPtg4jcvDSh/h+xO032/Nb4Qi+DpXFY8/Y8n3M5amvLtDsUW26sDoZ9+ZrdsoKzLUxj/L9cPJyzDGj0nGN3LbYD5g9+Pxi9eDS7ZoQDxRBEARBEARBEARBEIQakB9QBEEQBEEQBEEQBEEQakB+QBEEQRAEQRAEQRAEQaiBO4uBcqv825gleu5mzBB79/r/aspmG+hTXCoz9F+mayQWAi1zwX7aV0p1sTcqw6E5M11HqjZD8mVWzmzn3Vpqe7FjUwVS61rykALN6El0hzvuMM2vCb8p0vgxNB2hMnO1qj5Gi6OwnIb20hQYyM6VtInQbPckksLver5mWsPwnpTiqQ4pZf25ftH1j0M2y90yiemaWdW1FTvlZKddnKIiUIfE4lBKqYKR0E0WRKN9wvfavwVDa+gfrUdOa3aFjzMrZz86iGMwn01kx7HfQSub1QPxUIJxi9Wm7K0O2r6sT1YTo8d0DeMx5W3EP/DM4GMxvzGO4zKRJtDijvg7lrPJrE5uV2ioA76GJrZ4MNd2e6yBpjrwRD1lC+90rlUtetR+TIc7xXcHojykDeDnKv0xKzY9ij54ZjxiU5hyeRpn5yBcj8Y5yW3Go0kEnEDslbBlSLNpCvDXbIMXj8fgnoU6isSvUnk8Dkud49D1X+mNuCcsNsFCPv7jJtiOLxM7iWvfaTyR4G1XNJuuH9YGXNMeNoeM/49sfswdQcfMhVm8b8Q8j+cZcBZz5e+L8H716W/pMyTNR39t35LHzCr5FLaB7BUavYl1c+OJP1mdVm89o9k596Bd456yHxeDjsb7luJzjg3lMZbiyhFLij6D9488FaQ9LX/kbMQP0MdkuPh2e+VoPD/EHsBwfw47V5vYqy/h3dxL0kvW+Z731d1f2p7Rz3fkwT96DY3X7LQfW2i2xx7sIfSpzA1RiK2zuhHintAYTkoplReOOdGXjE3PzdDom3Upomk8L5q+OvkHHg8rZhTGYOwhzPFmkjq8oBHfL7V/4V+u5zYojcF+JOpr3lOMe9BWNGZZ3NOIh6bf6ZlJPLyyALRhXlP+FeXolDmafc+Xz2p23Q/RD/R7pZQ26Fe5f2C8FBTx+EzRIxI0m8a5OjcD/T552gJaRTUqQNyGcy3JOd02p+cjT2j2vt/QxyKiSGy9Qj53pw7+mhy9qBxFj3P9NbvWZj7v1WqDZ00djb/HzUXcLn3b1dmLNmpzAHPb4Swec6PNqyR1cRli0SR/jz7+eTue8ve5xEc123NYvmYbZ/LvZVfb4vitT/do9rMnEM/EaMynVViMFrpfzGnPx4/H51jDUpohVkifVzto9tLDrVmdPjtPK0dTGoI3n7iUf178R2i3wu/wblN7oA26jBnL6jg9e02z6bs4kLWGfzBOqX7vYWN04VO0p8t13h7+E/HOMrNI3L9FpaxcdjfsxyN+xjXo/SzNWM/qjOr7nGZHBiO20PPnT7By315FO35Xf4dmx7XEHrzhm3xvfOkJHsumJsQDRRAEQRAEQRAEQRAEoQbkBxRBEARBEARBEARBEIQauLsSnv+X5C7VyXb+H8bk7a3Z5vwCds6JSHqsHnaSFDpzyYMirnfsc4i7uVJKmXPhKu2ckavZFvKeaRmllHIKg4s5lc9YTpxj5QzOcHNnqYbJ8zlF1md1qtIukgugfcv7cFc41z/hBkZlOyz1cRF3tbxb0BS4eimIy0a4pepdrbX6umNDm2aabT0Md2K9ZMcUF40DIgmqagB5hT6FcOlAyADc1uF6/5Ds2JHKUdkOlfMopZR7DlypSwNqKXukrozX7LjXyH2TMq7r+bM6BQcpR0PbzZqawc4ZMiB1CP4CkhtjC6T4K47gLqRuv9l2wS4YZV/GQlMp02esyr7KypXHwf088AR6ksWJz4E01WkhkeBQ+Y0eq52f570S+TyUSlMEv0n6DnGNp8+jlFJ5Y3lKQkfgvwRu2/mf8Xfb8DlIG85FIl3l1c6YX+sc4w9M+xpN51t1bzwr55wL92iDH8pd743P0T9/QYd2ts/p5z033BOV7dwcivpxE/i1aV9MedRXs831+Nwf8htkVXT8Utlev7l/sTqrPu6l7gZ0TQjSdUmnumGa/TuyOrJ065aEs6wOnfupdCm7yz2snPtOnLtG5AHe87G29onifcm4Bute7QdJmuh/pIIk455IHE7dBxlR7mNcplkaiHFb5zjGz/g53C37lQN4jqptSA+qumO+uvwSl+zUf5PIZV5/TjmCpD/Qx0MVl/CkfkLSBodjNXMZgHmqvE8bVsc9PV+z04ZCytG3WW1WLnd8A82OHI5x0f4EJG/7W3PZXMQfFcoWVVey2XFhFFJzel3E/qloDfY3tb7m6xjdfVEJiusZvi+j97dvLEmVS1La79+QwOr06zgQB1xleccUh+D+XfP5DoSKqOg+IW8MkXDolnFrb0hJ6zyEnMynFyWwcr1C0d7uT9n+fmHO4f3o4mrse7zXoE0j0/l8FnEI73r/KshK/M8gza4+NW+4wpjo++Mgzc7pxPcV/pXYM/qfwY6k/ir0nU1HmrM6vQeEaPZWB6qwrq3HPs59eBg7V+9d2/LN7CmYC5zf5rLBG4WQZVAZVAtfnmLcSrrr+svYFzobEpQ9vuoFKSTtZWF/c3mdpRbmvfHhj2l27PgUzfbY6MLqrMnC5/bC0FQBR1gx1X1qESkXr9nGpniPzh58bkgh6bIVVx/fMZVB+IyGk/h3otLOkPAFzYOcNerbpzT7wKLZrM69GzCHv3bOdrsrpdT9E8dpduZL+Pv5/nM128Wg+55I6OEEyZipiKd3bhaD/ejp/ti7pH6ToNmdxz1Pq6hdSxZpNm2PH/bzPeHKSOw9uj0G+VJsPtozflsuq3ORK8hqRDxQBEEQBEEQBEEQBEEQakB+QBEEQRAEQRAEQRAEQaiBuyvh+T+G+QHufps1CS5RJ+77TrOb7xut2aFfcV9De1lE/gGVKhjI71QWs+0ySt0VyZO5EJkhqJuzUkpZcuBKbMhHVzC4wNXNEB7K6qgzPAuB9jk6OQ77HJJFx1CBd27UP78L7o/KDqwlPHJz0Fa8p2XhWzW70V647rn+xWUQwd/BVcsQimu7bORRyGkLmMLgAm2pRrZj8q9t99y/QS/bYZ/ZBC7IFle0nfUoyaJAMhQppZQix4Uj4Fbuu+4kK2ZOTFG2MFTTxu6bIH2i77BwOHdfz2uBNv9i8DLNnrIFUepds7j4qN77cC/09oR8RN+fo4j76E2SWchVl9WHYrUjSfs30HbLmcDdCgNOlmh2YRRchv23wVXVs6yS1bG6winaUB+ZF2of5u6HV3pwd37tHnrDpTxwJ3djVX8dw+c2RQTy3DZ+rBh1gz7fEdkBGjaDm65zMauiQj+x7RZa2NCXHdOMB8qItrfcyNdsGiVfKaUCD+UrR2Nsjud3zeH/W0hchPHmcQztVhZAevsCPo5KHkZfLQ3A9UI/5dlW8h9FOa+fIKep/T7mXvMP/F59z0P2U0gyEvkdzOIFJ8KdvWIyXGRb+CZodtp5HnmeSiajaqHdC2K4++31YbiHmyFw8a4kU+/2Po1pFXV9krorWCuxrnj/eoydS5sJmWZZDOb02NFYy8se5PKZy13QXtEvEjlcEc82d+U5PHdwf9v9/cZIPgfOazxfs2f2R9YKKrdSissi/14At+XIDXBHbjSLzwHnpvlqdnocfMc/+3QYKxdL5Go0Y9dVIrOoN49nNTDos/g5gNBP8c7SV3EJg/MprBWZL2OMBBAJhMsmvn6byfwRtotIcCP5PiZop20J7L4WWFPyH+MZ5HyTMMFVesJl3Xkb3xN+8+Q8zW73DO6n21lksChz4W1t2Q45QHztTNxbSy5H+SsBmT/qzkYGiRbetmUG/0O6cjRueWiDnBbcfT8sF1mNDPvRh468i4wg+ns0/Yq1i7ZH74ceY+W2ZCGb0v2nkOkjK4DMP635Xs1nPWQ7Bz+yfw+7f8M1qtpgbntn0nLNfn8Dr0PlgWnDMLd0eJCPncX1kDKQfm7KbyhjnMX7hOlqvrobBH9hX65Bv5Ok/4S534XIIhc24YvRiENPa/a1yUTqU8S/zxx5B+8+cuN4zfY6hzEX8nk190bYuWSx3XP0/VYSyWVRJ74utBqHufcoyRgUvXoCK2eegrUlFV8TVdw8/D32pXxWhwuMHENar6U40EnxuowhmfjI98jx9+zW7Domvn6nDcCa0mwW9nGfevN2q+iBdcQ7GN8nm66Yqtl1d/AnfmI2OvblvzC3BQfz9XN40DbN/j12s2ZnVGH87Vq0iNXJNZNsaCQ7W/8zI5Q9DGY8U0Zvso491YyVq3uc9L/3apapigeKIAiCIAiCIAiCIAhCDcgPKIIgCIIgCIIgCIIgCDXw35Pw/BfkKrY+y3J/vGbPWraAFWvgDPfKciscB4/eB1epxNb8PmdG3qtuCfp8VnPNZZS6K9l/qsvkQt2eaeYFUx7ctCwZOpdwO1lUaOR4pXgkdEPaZc0uvQ/341TC34vzWWTKsYbApfPrjb8oe2wvhTtWQvtvNHtVsxBWbsUCuCmrpFTNNLq6snLWZsgKYE2+ZPMzq83w81/CUAAXN+uZyzbL0AxMSnE5l3cy3OCoZEsppVRJiaoJ/bVLOkLiYKyAy1/QhDRWbn/sFs1Oq8QzpD4MWQh131NKqRHnX8B9n4aMyFzJI59Td1qLM/ppddk29JmpHAHNSHUzgp8L2YrP89kHN18LyfpRFcvdzZ2NcIE0n0tS9ggtg3skdaj0+xZujqXduVu6czqybpi90Q8Ov/8VK0fbamwmMqnU6wh38z8b/cHqNHKHW2j4W3CN9MjkkrzsaXD7DZmPEPiZz8L9Nuxj7tp7N1YPy0lkRQo7WU1BWqdjvGZfm8SzltSZj3u+/grOmbu0ZOX8DmCOzR4PCUXgdLiiZ7wZweqEv41rf7AKN9vFjYf8P1kBidr313HtX//GOpayeSGrQ12grWS698jm4630b5qBCHOG1YhKZXE8G4V3qror0GwsellHaX1I4uJG25bgutzgzxb9YoJmX52KtgtdfoaVq0vc7OmYK9gYo9kWK5c+jp83RbNDLmE+MHzMNXA7m67Dfe+CxDGtH9zXIyvH0Sqqdijup/YncNk27uayMUrFt2gv/25krujHM9wU1LefaeFOqewJeVVVJd8P0MQONAOQoRXc1amESimlSgMxM4T/ifnw3qXHWbl9kyGP8liEPpq5EnJH9xw+lpgbeB/c3JnveH+bewOSuHaukMNub/y7Zh/6mMs0X4/Euz7YF7ahF5/pbjTA57q0z9fsv7xpNpVCdbehsp3gg9wtn8p2BpxF3/+ntIjUccJXkcxX0aYmfmk1NQvvJjM9QLP9r5P35Mz7kfs1jMzYnaPx92nurJxPR2QE2d8Ce87HL96v2Vuy/mZ1YpdjTnUjaqtd27kcLcoLEtQLl/E9pME2yPFc3Pn4r8q0vf+8m6R8hnXBQpJL1UnHO11wtSurYzoNiVQZScZZxRUjqttjYzTbKx6ynZMv4H18PDqW1Znpj/0O7T/3nXiElfObivbPfg5SqhPTce3lhQGsTm4V9qKj0rtodux0LvXZfBEpkLqNwjO8tRp6nuG7+DzsllpX3U16nXuQHXucRWNtINmFmh4Yqdn0XSqlVLvpkCqFrsD8uoXUV0qpxvOxjwsm2X9YuVH27zVnGN7z1z492bnHvXP1xZVSSoU7oU81ncOzOfkloS96XMS+9O8/VrByb+dg32/aiTattxNl/rjM9wN9Ro1Vt4N4oAiCIAiCIAiCIAiCINSA/IAiCIIgCIIgCIIgCIJQA/IDiiAIgiAIgiAIgiAIQg3892Kg3M2YJzoqe0Dn/8ICpNxq5Mw1vOVW6FAvVuH+iiyIjdLShetgK3oTrfVf0J5by3VizTvhbqQxriYVLsV4FSluq7KvVlOS1PGAyJHGPPnHPZDYG7W2IMaBUxiP9ZDfDfFRHnkD6Yn1yusscy1li9euQtf8aTDXPP/Qvh8+Nx8xGMpDeLpj10Q8e9WNG5pt7RCPv+9NYHX06XQdBY1LY7zOU/PZ08c6RYSjDIlvoZRSRndofs1O+O3UTJ5TX65gALS8NMWqVZfe2v0o4sB8fnCdZp+tCGblZt+I0OxpfumavaII4tlzpTy1WH4sxqPPEfvjrDwWWnb3X5F3z2Kr8F2kupTeacPx3iJX4M4qw5AK27ib913lh5TCRSRlrdfqg6yYOQAxKUpbYiy4rYOG92obHu/Guw6ut/8zHguDcqwc901TMlJonAallHK1k/nbVMzbMHi27RTY+rgnFPoeHMXI8xhTK4f3YueyHkCspZBZuK+0hxFDyfes/fm77geoQ2NqKKVU6DWkRQ74GjEoKrtiHaMxT5RSKuNNXGPicejrz7bnKSY9DLYTKqYMRVv/VMRTVicuQVyKuKcxX/Pk4kqFHML8f2kiUpdW4XFU+Dv8vgO2kwP73e220cc9oTSYTNZp8nc6pxv0c/oOxJQIexoxasrW8FS+aWcwLmqfxtzr35ekCY6KYHVKYxBvZfMGrtumDEjqrdnDG6MdaCyAsSd3sTrL1yE+QQCJe3LxHZ5Ovf6bmMsLytCHaU9w2aB7p5N5v3UEWR2xuseMOsLOJX2L/m+KQcpa81HEofH5IJLVKT+I9cY5D3F5ssp8WTmnQsQGKu2MNb+OF+okLYxhdaJHntLskHqIFeEynO9Qtj+BuWnxI9D8V3mQ3mfkc0WswlzushHvvbwvj0NTdy1iTiW/jzaNWXpFs68PbsLq1P7GfvybO6XVgNOanZjJU5WbhuDd7L1B95/YZyQu4GnDO8SjXFV7zBkmXersC7NIStVn8d6PvINYF/p1yO049iZJ32BfuTA+jJVzNZJYSd8hxW3iE4gH1vxzHo8hyk7a3Yw3+FhxikN8kwF9EJciLBrPUOGpi0X3v4C5Nt5BzyaIF/f1EPShmBXPsDrR7+IdjElEzLulcXxspqxA7C+/bej/vQYiVXXac/x/+vtDo8gRxulXDVeycnO+767Z16/jGjR+R/0N+azOm78iJfaWptg7PZd8TtnD5TiCeF0z47vDe+3XsXIje9E9YM3pcG+F5p/hWegeRCkef6vTJKSIvmcGnqX7iKdYncLJiB9SEoz+Gr2Dx2gL6YT4KqUPYdz24l8BGIlfoVzcM9h/Ris+F61+COM7sQxxK1eswTp27lkeu5RC9y59W/L4KioA55K+RUwVnyMYZw/yKUDVP3Br35f/g3igCIIgCIIgCIIgCIIg1ID8gCIIgiAIgiAIgiAIglADBqv11nQjPYxDbu2KRuLoa7GTvtcB0DSqxfc3ZOee+wLuXT3dIE1xNnAn5EqSXvjNa3A73LEQLpi735jD6tBrNP5hsmZHzXSAmyRJEfynefW/v57i7VZd+l2a7thQAvdW8xUu5zFG4xqWFJK+18K7EUuRTKQ+1gq4CF56vjWrc5q4at0w20+le7EKbfBNXkfNPvox0p72e30nq/NKwAXN7jgVLm5eiTyNreHyNc02511XtwKVIm3KnFNNydujh9MwHOjGEnVtpal4LZ3gfud0mLsiFvWDm73HL3AZtrZvwcoZSHpdOs4qW0RrdkkIdzd9/YNlmt3b3b7M5lA52r+tC9xX73kHbqH3j+Wu47ND4Nrd9mWUq32Sp2s0ZsCN2UDkelSSVjKIpyGnUp8/LT/bve/boXcdpIdTJj7n5PbBO6wiGbSDd5O+ppuSy+qiDWrdwLvNjfdk5fwXkzmIzCWJ8+EGHjfxEK3CUtFtLcE76+nO02y+l4s59hFvpIR78NfnNXvtoNmsjqsBffbRT6ZrtksBF1X5nSZp03Vppu1BpZS7Ns64pTo1QefKsge5i7nr+kP64rdFNkm3GvwFd7/NHQdX/IBFaMOrU1DH2IPLwkK98c7Wx23S7OjtT7JyKd2WKVtErcEcmDr4a3aOSkeuLoEbtu/39tc4mgrd3BYu/YWRPE18PrKdq6SXn1eOorq2o9xoANVyvdUkhfeVbFbOUAuyTCpjrPAwsHIBy5EGMfNFrGd1P7QvP7v+JNq7wVjM0TNCN7NyUxMx/9d2hQSg+H7IZYefz2J1Rntj/TpajjX41VguKbgwF2ulcsV4dEvDc9c+x9ec7HZ49tTnX1COIO8yfKhHNOzBzlmK8cxUVpr0NtarqHU8JbpeivUfSh7m835uM8zLoXswp6aOwjNObPsXq/PNGsj6zo2z71ZOU+1u2YQ+EdoObXV9PfcdLw4n0pQk3EPgwtvfV+olY1YnPOvm8x/d9vVsQcebqQGXOmUMrKPZZQFYy1zz+Nih7Jz0qWaPrNfhX93bxbd5Xze74R5iVuRrtjH/Jiv30GaM5bnnHtBsvx+wzu7+ks+VzQ6O0OzwZzBH62XwNN29cU+CZtuTOuhx1N5EKaWifvxAs3128fn5Rgv0Q490MkY+xXxWrktvXumBcgVR+J/81CfWsXKfHIfEImo+/m5vzCqlVJsEzEGrz2HOihx+gpUz+UMCTffts9Nx37OvdWN1kl/GOkWlzaen8rE9JgPfMS61Q5/JexrzuEe2bn9egfe4c/NM5Qhif35XsyMePcnOXZuIPj/smT81++8BeEb6fU8P3Qc+mdGJnctqj+9iNwej7Ss8MZ4L4lgVlfj4V8oWdG+hlFIjgrH/fn/JcM32P439Z87T/Ltg3Ucg4Uxajj5hyOOhFGKeg0w1ex3CIZxo+6Nmd5g2gdXZOxu6YmMwT/tsC/FAEQRBEARBEARBEARBqAH5AUUQBEEQBEEQBEEQBKEG7iwLj0Hnikddzu+ibIdyaTlcII+24e5CFpZ3w2Dn7/z47TpwJdriCdespy/ybAzLIhDF27ux/Wwbd8RdzlSkd+EyBSDzya1m61FEgmP0JFl4qpG7GAPgXlfwNWQCmxt/wso9mdFHs5eF79bsvWW83Tq44ne/uaGQe0Q9iOwt2681YHUG+0B2UPUU2s0wjb9zu89BpGkmby6dsNzIt13nX2K9D8+jd3Oksh32dzfcp3PdEHauyhVjwdwFrm9ONytYOfpGLHHILJH7Ityld9xDfDCVUn4muFhTF2baPkpx2Q6lLAD39tuhe9i59/rv0eyc+zC/BOzlrn2lbSCPqbXZdlYOU9ndz8lT0RQyt+JQ7lZ4gyQsiJoB92w6a+qzOjmfRfvQtjE05Zk1KMWD4BrcvRUyJhTtrW2ruFJKKQ8jXNnveWcSO3fsDcyxYzPhau93Du323BheZ/v3SzV7yHikX9k7sBErV5WartnU1d59Lc8yRCkOdnwCOUMbjDe9ZIe6xVuvI5tE8kw0aNSv3CXcehhZO4L341zxYC4noLKdqcnnNXvyVrRhWutVrE70DiLVIe6zdQK4rG1gEtavdbFbNDt2Kt7tm5151o7fYyEliWoLqY/HlVasnPM2uLxfmoZzdbdhbvL78Qyr40uknepl5TBynsFYCPyKyx5oZjK3bMjhaCYzoxfPxmYpQgop382Q2aRP5u+q6GHMVRE/QBJU5wCut/M47+99WiPL1oKwA+QMd6ff2XSdZp+pwNw7aStcnT9Yy+eA0SRjyL6SWM1OWsYzmwVuxzoRuO2yZlddgq2XZnj+RrK6OUh91WodMlR4judyx+CDmN83/fytZj/wFNog9WH+zr5ejjH34RhkY3HLLmPlaj2Bfmj4G2mjWsbgGVcs4ns/Y2f066GpkANcK+F953Kur2ab6+FzrhVi31B5H58rdrSHbGDjTexd1i4MZOWS5mLuiF2JPpE4Gu+h0RyeVc98rmZX9NvFKRgZ785PCGDnvJCoREVPx1ikmWn02blGfgTZDs2+tLcrl0QPmvmiZtf+G2214fBGzR6cwu/n5G6Mg64rMK9Pr53CytHsVktSl2v2m4NwP42a8Sw8kT9CqrPhGObXT69Hs3K/fIj2qX0ScxCV7Vg68+wn61cuUneD6JGYf6h0Qyn+DpJnt7NZruNUvn51fBlz2OlB2Pt87vUQ/9xX0RdyJmDeKp2GuSl8yClW5706OG7kBgncR9MfZeV8UrGD2jMPMqteoehzW7L4ujDtE3yXcR6HMde7Zz9WLvUg1o/ELMyvbV7FM+j3CzSDpqMIW4p9oVNdLgEM2Qr5+o4F+F527TfMbflXuLTV/SL2UB2eRZv6nObfgYzkvec+gjmZyogO6/pRk3kYJ1TOmrKSZ+V8/QBkqgZ/7G7dL2KuLU/ne1b6bl3dMb+aM3hYgSqSxbBtCPZV970A2c7Vjvz7X7+OAzV7U7KqEfFAEQRBEARBEARBEARBqAH5AUUQBEEQBEEQBEEQBKEG5AcUQRAEQRAEQRAEQRCEGrgzIfldjtVhDxq3YVX8l5pdaeWPYSIxWp5MR+qsY9t5uuNTY3CNv8ugLws6DG1p8nVex+n97er/L1hLSm3+3RSENHTWAq6pt2RCD2ctt5+u1uCENqHX2NN8FynFY4n0qQ1N3Qe50CR+8+cDrNyOIZ9pdr4Fn1N/JX4PLAzlGsG496ELvHoVKYB907jm0h5OYYgnQrXzdxOnfLSPPrKQU726mm0hcVuctyLlr0GXtvpGQ4wL/z1IhVkWXYeVc4lB6lKVAk388TY0raO7skdxFbSIDzz5NDsX9+4Zm+UCT1RpdmYg/13X00h07iSEyfV7g1g5nxUHVE24J3N9p7ldczsl7xzTTsTbuaFLqRg1w3560/9gaBDFjhOfQX813cS7iX2fx5eoJCmsr7VBuS4uGH9tvHncpi6nB+LaRrxcn/QqZY+DV6AJDqGpk6t5l2vS4zU7MPUCO0c1rZ4p0L7erCYeiv8xrvN3BEWRmCM8dSF0aJwWist1jKkLT7mxc3H0Ggcwt3m24nE06Go6ZQviNqQ9DC13zI883V7K8IXKFh2DUtnxuq3QsSuEAlB5Y6Hf/mE7X8/fHoZ+ZSzH89GYJ3rCPka/ziNpemsf5fGVaOwER0LjnpQO5Fpvt3VEn54O8+JqaO89tvG1iMalofGm6r1nf/xeHYPn7uuNNJvLHtrNykVte0qzO+aFanatWVzrvXzJbM0+X4Fy2fmIuRF8kK8MI+7HWnnoANbQmBf43Fg4Av3iwnP1NDt2Ga6d/CrXlMe+wmNqOQLav+ouOc1P/oY2obEZisZgzY97h8+Bn7yINjUpzMM0vpFSSgUPRFyb6+sRRKjWENxP5VOsiirJxZp35QfEhymqx2O3jBiDPc7bgbi/bo+N0ewqd15nVmwXzS410zhhfI/WcB7WbXMSxnpgQ/S9Gy39WZ3m3zl+v0LT9Lrk8ngfq19ESuLJxxAXK3I+4hDo9zPjEvEsM9ajb/b9iKeorzcB5X75bINmd5yCmAs0BoZSSqnobbYeQTWez+OZ1I/P1+wGzhjnma9jDT83Xpe+GiGiVON9o3Cfg3lfbn0YcUfS/iZxbQqxNht3HadVVNtPntXsU1/YeIC7zPt9EHdreSHiynj+xu8zYQ3sKy9g3x3xKp8rL76DPurUFM9tsGDMfZuxh9V57RrqHO+P/WzotxmsnNNIxDOZfJnGaMF3lL73P8zqbPx7rWb3Oor9TtZGvmdLfB5t3mD345ptJvHsAnXxoq7e66scDd3bb9DFHGk2G305uBeexfSjn2anvcNjhXYfiQkuvR++//r8eY2Vo+nk6/rnazaNi0NjniilVElshc1ySvH7jvsb7zPxfsQd6vVSvGabPfh6bjmBuTv8Y8zrxnyeOjy7G/Yah35qodlT3/hVs7+cN4jV2bBnHTn6VNWEeKAIgiAIgiAIgiAIgiDUgPyAIgiCIAiCIAiCIAiCUAN3JuExcvdDu6mLabrjO5T90NRec75F6tT6RCKiT0/80Pkhmu08GOnifPvxe2j8/WTNjvsSLmHGS3BR8+Pet8ryPq6xuvk3mj2u67OsnNMO++7O/01oKif9PRk84LKuSpCeymDE72oGP19+vSvZmm3yg3uY+QZ3qb8xAqls33hjmc17O1nBUwtuut5Js69MidBsn5Y8bXb3H6Zrdv1NuEat3XBx82/dlNWptKKPrukCV7aXW49l5Qz74Xqt2sI9rOoQpD5UnqSUUkZfH3U3MJ+5YPcclRHp70cro0tbHfE6jm/2h1uc6x88BVtlx3jN/pO4Oc68ir9PD9jL6ky8OECzix+D67WlCf+Ndt9lyIOMu3xhE7VR7LPc9bNyMNoubSBS+/X8/glWjr6Ha2PR/0L+wNiuSuSpC2kfvhsYK2suo8fsw1NzNpgBd2DqTml15zKqCh+4fic+jj6+qADu/+N8slideedxjcCvYKcP5XNlo0Vw0Qx/B3IaKsk4+zi/b/Y5TX/U7HcUT1Ntdof7KB1/7ieVfZIzqjl5Z3iurlkCpufUc3Dv1buxUqzt4UKq9p2wWy6VyHbSKrF2fT1wMStH0xg3nI4UhklfcDmexQXt2P45yID8V0GiEqCTFClkFlRuV+3/j8VE5j0Dsb0zKmwVV0opdWlYtN1z/4bErzCf1dvCz1V2xxpYayfefcAq9PdKnSIx42fM/VHPQfZ39lWdNJSkIf35Dbj8RjpzSRDF3RNu5eWrMH68svga2mnbNM12TccYqUUy4Lqt43PlygUJOIiE5LLXC/GsXG481tTaRMVqyMD8EDmcy3fti/runOgXMeYmJCWyc/NjIa1Jmge3/PBNuBNzIb9HmgL+i6Sdmj0tgn9up5PYN/yciv3sxuNbNXtDyd+sTj931Om8fpxmFzTmb4bKdqj0yElh/1X4FE8/PTvkiLIJn65V3C7MIx7ukJfnX8Ea6RdawOrs/p2kx21t+2Nul3YnyMI2toideywDqYYriXIqKAH9vmBUO1pFvfYDkU34YT8fvIdLbj+eAff7BmuQSzt1nm1Jo1K8DeiebuDXXC7ywSQsOH27PKbZpS/jvhvtfYzVoWl3j1/GPveeV/j3AdUG47R0C9ZpN5Ip26hbz4PnkLH9xXPKUaSsQH/oFcrPFQ5Hu3w8F22ycjpk8ysq67I6iUvQqeKexj2nfcj7eNx96Zpd/BGuUWsztK4d5r3A6sROwV6jvC8khDsb8fUwZjrWtnOHsOZFbsPaqLqnsTqsXxBK4rlsLmoNdFqGKsybbnmwL37A5Y5n7qNyGcfkfE9/D+9T3251PbCuWT7BHtF5ONYr/fMWPIl7rvcnxrO1jIdmuDoV/aD4CN5t65+e0eyyJvw7eOPXMXH1e72vZtN040opVVmM+ZqlPlboR3O6/cDqlCVi/CyYhn1+v2V8Lfw1M16z97f4RbMj12HuTnuNS/JafIJ7ODVL1Yh4oAiCIAiCIAiCIAiCINSA/IAiCIIgCIIgCIIgCIJQA3eYhcdScxml7ki2Y9BJL64+B7fJOOKeebwC97CpkLuEX18F97CAG3BV9vmBu2hT4cWduKeGmHA/uVNL2LngHXdwwbsAle2YaHYVpZShikivENxdWSvggm3O4y6UFDPJrlPerw0799CLeAHU9fVoOa59voK7Q59ZiLb3O4x2CyrkEa6DfkKUaOrCyzL/HOFR0H+5iSjirkRXUe8LLum4MgaZCyzHEO2Z9mRrFe8tBg/7GWn+DU5h8NMras3dJt1+g+u4wQWuePp7Y9cLCdZszwS42OlrRHyepNk0ovmXYXCnTNRJU05txXsLT4MrnatORhS63u7t2cXZYLL594t9+XuPOo0sKME/E3dwT0jV9JIdg5/j5Vf0M6rL2mEPw94Edmwk/cDcHGOhJIC7jWb0s329X0chM0erNVxOd/MG3qFzFFwj457ez8rRedlKJJtWP2/NbjSPSxBmd47Q7GsVKJe4lPuRx40h7uvExVoZiXTvgE7PExehHA7JIpT2kAc75UO6U+1leDfULTYigme7oOOqyh1zU9mj3H3d/RrmxJiVOJc8Am7pkc5cJmu4ira/+DiyNsVO59Kmqky8Nya5JGUKGngpSvyHcGO9eY/9TGvpkyH9CTiNpy3zxXjVC+QKG9+Bpu0WoFKarBk8i0Lo31h/ih6GnMfzZ8xnpiYNWB2zKzLiVF2Cm37sD4GsHO2v3X9F26U8irabfSOCX9tM/m9FurjhMs8iUPcPzE3ua23PI9nPtdf9JUGzyq141+1P6GRVY6EDMpbblsSUEZmnUkrlPVms7iZvzhnNjitehm0qxQrsnoY9iV48nvs42ndaBJ758kv8Pe1ujvd5IgvyQjPZ286N4RkY+5FsEplD8M4afs5d/jtsh8u/p0Ifu/IC7mHYEzybY8v3MObqLMC9bdFl26izBjJJj1/gtk+Fe1SyppRS5bVvcb9+GxzuCulZ0tu6/Y8Fn2f1RAsFLkT/WfH+Z6zKxPodbX7OhS/vZcfPP4RMRpGfYw/TaTLe+aX+uh3NV5iPqLzvaEv+/+OYzyADiU7Ed4U4qCVVwUg+dyfPwrGLIUGzz0zm0oBeH8Rrtnu/TM0ecR5rhr/TTVpFvTZblwbKQUSPRIgCKnlTSim/oyTLE5E7P7+b3st5RXFyx/s2xiM1jamcS+8ru0BOs/UyNoL96/M2ptD+T2UrUWvHs3J1SGKgA59ABmtPpvPPa6NcSle+R+pwEpla9jaHhIwsK+rjbg+xOh1b4f72rbZ7C7dFRSjmc/28ELkBoQj8gjCHBw5AP6bZpJRSKr43vt8UjIDkdO4FnrWq73JIvqJewt6HrilvBp7lN/sozOnZqN/6jWdYMZoZqMejTypbDPAosfl3pZT6Zjv2N9sf42Nz/4YVmt3sC8yvcZ9ifu33KW+3E3vpuK1ZNiceKIIgCIIgCIIgCIIgCDUgP6AIgiAIgiAIgiAIgiDUwK1LeGjmHXtZd+4QGn3a8nE+O3egAVymLlXBZWjay4j07bebuy0HucMV1rF3ap8OYTzCc4qdco7ITHQ70CwZVclp9ssReQfNtKOHuvxZK9Eewz7l0ZXjXdEms28g88L6yV0121TM3blrWxDN3UD6hPlCst37MXrB/dzoD4fxqnTeJ7q6w1XSTN57tjd/J2vPcDdK7V5j4SZvTkpl56ouZuqLOwaSDelaKy5jqf8bbJaZ5T5E6XdKvUKrKHNOLsoRqc/19XGs3JZ68Dm8UoX30WTeDM0O//IUq+M1CO+UyT10UiqnqAjNrkpN12yTNyQe+swKZyrgIl1sxZQVci9/vrxBRAL2HVwNq5rjM5128H5R8gB/dodApCd6aYC6jLFliQ3X7AofjKtrrbk0J+wjuBwaLsNtOeNrLptL64/I9DRLi2cnSJteb92b1fEdhs8NXIjPcarHJWNVunb8D4WNIXUwVfD57JtvEH395AtwjfygD5fjdHwE7q7eJ4kLMR1nRF6jlFI3ou+CbI7IhIJCuVuxwYJnq/gTKaNq9YBE7WoPLkn0X4y+5rwNUsoPUnk2tGwzpBoDPTDe+vREOhzLae42Hd0Obq1en6JPJATE8nIzLms2zZRGpR91f+Lz2Yajm5Utxh7swP9wr21ZiXO3Vjb/rpRSceOQcUE9bbfYvyL0E/uyOZ/zeNd0b6DPeOZ/Rtlk68/fsuMxGZAebAlHVo/7n0Gk/9zHuAuy606sWde7Ym6r/Q2XwLmvhfwj6Vu80/o/Yn7xyuA7nG5nkQ3N/DnW/p1LedaKqInI6ODqib7tsRF/r/0Nl/GF/UEOHKQKMZJMgMWhfP5wuY7nrPIhMqNzkJiWPsRlRgHLMbbCDsAtPfEslzDpXeD/Q+NlkzT7waMH2Tnq5u83DnN035VcjrO+CfYhtN1MtdAPflzejdU5RbNBvEY+c9DjrJzHAdwTzZhSGIm9Qt0PeP9feJFmzONZTu4UKu32q8/7bdDzpK1SMD8aXSE/mjSYu/IbXKCRTH6PSvJ5R8vsjXcb1h0ShO1Zv2u2Xgrw56qlmt3rmXjNLt8awcrFDsT8byVycN9d2Js4PcL3i/Pfw6Do+8AIzd741xpWztQA0luLF97D909jz5LXzI3VqfMVace5jsvC47UbcvZLC7k83iMbe3InIlu1nOTrDyV2Kva/nx/B+9hXGsXKLW6LuTJ+PpGTIKmmip3C+27XP7BIpM+hX1f5XGEqx/HEy1TKAZ1Nyqc8K1DfZti7OO/EeG57nK+hTsv9NbvXqnjNzvkde7vAi/w7iodOuu4IgkMwzprsH8nOxY3FuvrgGZRb+DJ03fXe5e/2+mbIT1MmYa5870ofVs5DlwnsP+xrgb1j71b8fkxX8zW76hL2ID2P8+ydPYZjrKYOQhvE6LLfUmg2wWvfQGLsdoKPHzpfV72FvztFYv92synWSKWU6vAs9qL7V9m/h/8gHiiCIAiCIAiCIAiCIAg1ID+gCIIgCIIgCIIgCIIg1ID8gCIIgiAIgiAIgiAIglADtx4DxcFxT2gskNLOSIe4pSFP/2UmUrcnnn1es71+gxa06r8QS+Q/VFrxHkzkGSottlOt/oP/4r0qpVRV9tWaCymlLDdvLUUhjXtC0+VN8F3Kyt0wQ5v29gBoEp0uIJYCvZZSulTBt3Q3Shm9oSfXxz2hbC6G7u1RL8TOSCoN0pWE5rayJ0m3uhWpVk2BPJWlOSdH3RVIX/E7x99IRW/Ev6i1+bCyRWnL+uzY5a8CzU7+FM/2VUPedpQR46C9rbsZGkq9BN6pDPdH9Y+lPXnKWvYeSVyZ8nrQNtPU20opVWbF2CqyQD88OpxrOn/8DnnuSgYhhoVnIp7bYuTj1Pug4+PXsNTfq3msDqenoKe+3gS6U79vSbp1Px5/g6VeDoDdqTmP2xD5O+IuxE1AWldTY8R5MV/nevWQnXk4R/5elclT8toj5x78Bh/xKo+ZcNJOnIEupweyY49fMJefIymOfY8hLlPoLzyqVO2ku5MO9z+4/3rQ/sl1MGlaS//F/PlpfAcap+iFxCGsHE2N2GQe0u0Z3kG/DRukOCReS1En/DmugS8rVvgIYkR4rMEz3axHUo16e7I6L1xBDIJTUxF75v6vDrByGQpjMf8x6MvdriMGgqsulo61QpdO10HQVO7lXZqxc5mPkzU7DfdsdsOcFfsWD3piKUI8rsB9vpr9+MX7Wbnl9f/WbJpaM4RkB63/Fk+rajmNeavOfPw9a7ou/TJJtxg7ms+J/8EtlscZOJeK9ez5z7ZqNk1pXN31boxGO156md+PV4bj0+HScRG7gK/f1kK0AY2LFbQfc+j+XXw+d26G+SOTxAkzBvF7b7xvlGbfE4q5LuxvvKfTr/E6pr+wvhSRofDFXzyuVAgZ3qk9kVK1bw/k9jw3haePpXP32l7zNHvLr8tZOarr9/4RNzHoNN7VX0ujaRU1+DjiSJyupxyCgcQIGRDO42PtS8KzpX2I/hT5MubHpCnOrM7DTfDek//CuDT68fnCMwrPueXZBM2m76X8IX5te6lsXXqms+OkzzGXR7+Ad5uyBLEuRm7fxOoMWT9Fs2MvYH5t8elEVs70ANnLJeKZ6F7H25vHNEuaZz+977+hqBNi4fmoXHaOxgaadwP7x7nrEc8s6iW+DhS3Q39rVAv7nadeHsjKldfGXsG9BO/D/xTaNOt5PueEzCJzIKYzZekYz8pVeqPN54QizkaDuWiH2Kl8fab7nY1xCZpN0+4qpdQuEoaouAptcqw1xrbSbZeqS598p+SeRLJya90yu+WWJJH4Zh+jTyYu5PGi6B4xinyFSOvD+2FBD7QV/eaTuAzf//z38PnsyLsrlC2azuXj4vSP+L4/NQufe+JBeq8JrE7V49izxiEUqnJayGMiXi7A3BN0GPNLFYlP46qLVUPjNN0K4oEiCIIgCIIgCIIgCIJQA/IDiiAIgiAIgiAIgiAIQg3cuoTnLtL0nZN2zz19sZdmu607ZLcc4y6mCnY3wlWJusU6G6uRON3q/dBydwGavkn/eTSlLC1nLeJpfc25cJ/yfB3pqVbf9GHlPkuCH6sfScFp8kfqMFXJXZupmy51ybaWlyt7VF2BRImmNKYu2Eop1dU9XbNdDHBZj3Lj8ptzTeD+ZvwbrqkGkobOUMCvbdK5UTsKmv7L+8fL7JyxaUPNpo7GTgVIi2k6ziUexmC4AHpEQh7Q0527d49Ie0CzXXef1eybD8N90WP9cVbH57cE3HcZ3AsNcVwiZYqJ1GxrFtrOpRD9TD+SWrlgzK0rRqqyD09w1+kIhXmESjAMJEWy4Z5G/OI5Bcrh0JS73fjcRnu837e25UPOxdx1nKafbboN93u90oOVG9kOLqrrZkLXEfYx3GBznuGp/HyT4E6c+QTOxX5wlpUrvRep/cpqY9kI32rflbThYrhrnh8LV83CMp6m2XUYXKfjxsA92BjfWLMtwf6sjqHE/pxwp2Q/i7EfPIfLw0yN8Pw0jWqfGZBw7FvB3VipPEFth5Rlb6NfWblvCzEuXa9jfXD/gUtrKNS9O3o12rCqFv+fiPd25KK0tIRUNuZ5vGdDRDirczofUgWatpfKHpRSqp7C/Oj7PfoeneOzvw1gdYJG85S+jsLSGuM6/SH+DuJGQjZI07wb9p9Afd310n5EucQLJHX6Fi4PmPoi5ttH7sP+ZGsixlKFlx+r4++MdrAeh3TIJ43PfNTl2i0A7y3gB7jJX2/It3AzO0K38sUppMqd0om7Kl9+CX3diTSJ52Uid6rg+xHvlcR1/3vlEGgq3iff+J2dO1yItSKDqBmuTUF/NT/G91OxS5Am/tJncMdOafsjK0fTA2e8jfbJHIS+E7eVVVGlHyNNeewZzN3nn+MytbLatvdxNFW2ewaXKpyZTKXrmB9pmmyllAo/iOfNuBfzy/ffYJ/s3oP35uZ17KegvVOujYW7/a7pfP9g7ILPr93Ctrw59vFj7JiKgB48ivG6aTOXE1h8+X70P6y/DCnMgzybPJOlMGnFdt5u0d3Qv9udwDMdGQB9xrprPVidQwtnafbMNmgDVayTk3fDNaIPo1+efQPP57KRS7EbZGCPpyYph9H0KPr4/k+4rKPraEjgnIncOrID5jnDPY1ZnV2LFml288+w5oeO5XOOd1fsYTNfR//v9SzmYf2e5i8vlPO4hL6f35iP+4g/sAYOiO6g2bFl2Ac66da5HhuwN2u0EPc9duhmVi43B/vHUDtf0eKW87TcUa2LbBf8F1hCsdcyZnKpCZU1bqmPua7hm3iuZT0Wsjrv9HwK13bGgxWF8TUl5nkuffoPLaLQp0uf5KEiei9Hv7r4CvrU/YP49wY6Hv33Yr8T8nKyzTJKKVVrMPYXnVfv0GxnA18/Fwfje1nB/WTnPQISrXNdlrA6D0zlEqOaEA8UQRAEQRAEQRAEQRCEGpAfUARBEARBEARBEARBEGrg7kp4qpGu5I+C6+arQZ+SM9y9+/ifcM0NV9yt2v7nkt+FrI7NHlRigasYzcKz+Sx3a4tVxEXxVmVEdzlDT5Uu4jCDZCexeEEmYdHVodlfFkZ8gXK6y1VuQrxmp0jiYl7dPRCqk+0wSHYovWyHUtfJtjv8Zwd7suO4M7azEzhVkCjO+sxG2equQyU7SillOW3bLbe0Hlz5ap3l8oqsgQjBPzqGuylSjmTA1TGyGO6V7mtJZhInPnVY4nF/TlchMylz41kSXPLyUYdIHIwx+EwnZ+4mn1iJqOoDiYfnSxe4uyeVzpT7Yx5x2QDXWKfr3AW4NLaOcjQloRg/gX/zjE1lE+AuXtAEtufPeLd6V97SLXBlv1oOOU8f/1Os3BsbIZurnWN7Lgn8Spcpxh1ygKht8OW3EjmcUkq5HcO4rUWzTlUjO9z65CeaXUCylEX55bFyJeQx6GxtSYCMSJ/5yuFZ4ZRSziQzQEUvnj2q1ha4M1+bCLfig8Ooi3qSoiR+g2uEViIzk9nKZ8uf++J6Xo3gakr77fWnuPTKJRj9uMIH/a1WEXenz++JDBLeiZgfy/rDxdZQxfvKzkaLlS1ct3nZ/LtSSl2bhGcIWoI51G9WBCtnzktUd4NKL8wZMQ14NH7alh8v/EqzXxuB7CRUzqOUUpHDcUwlAJU9eb9rsRBZOGoRNWDwXOxVqrq2olWUKSdfs4tJxgOaJUkppSa8iflxRzPdXPef+kt4P6VZymKD0De3lvA5Newj3B91bbeSbHxUrqsUX/sdRdVIjItFsx5i52hWq4w30b9eHAYJ3E8T+rA6G/9eq9lv52BP1uNcf1buT5Ldpnc43qH79BC795r5OMZW5AJI01KGcdf4qG1wje88Dtl1sj4mc+BMvpdd9Bhkc780omuSbcmKUkp9mg7JyfQI/D17GpcH7TtM9g7tlEOg6wjNeKeUUuVkLPr1hSu+wRkSx7IeLVgduubNDsFcO/vJI6xcp8nIdNXr13jNLhyBB3st+VtWZ3BKd3KErDMfR/3Cys1si/ngQAssShffgdTHPYtVUWeJ5CQhB214tNVqVq6Xwr3+uR0SgqhNeI9Fj/LG8T16axk0b5e/56O9bjbg63dZPRynfZug2QfKMKc/tWwKraI6TUKbuI+9ptkXUvlYWp++RrOfj8DfV8ZiXvHz4/29IgbSIXMjzL3Rw7g0Ouk7ZI5z7ob3W/ss1jaatUoppTLLIAUpr4Nr//oml2nFkiyBNLRAk+EjNTvoIF/TrUd4ZipHEPMY5C9bdBkOo36eoNn9BuO7zvnDkAbec+RRVidoD/ZXlhLs/XJ/bcLKJTVHfwnai/4xIHCDZs9990FWZ+oj6zX7y1X4e3rbUlbOnrwuaQ7GgnUpD/XgQfrITH++56L8dgz13DqjX/asc06zP8jlGfv2zCOZldSLqibEA0UQBEEQBEEQBEEQBKEG5AcUQRAEQRAEQRAEQRCEGpAfUARBEARBEARBEARBEGrg1mOg3Elq4GrKVUG2rQJMOPirlKdnivoOqa+4Eqoa/qU+3uiKezg/qzk752JAbJNHUxE/o+GzKayc4xX6jsde2l/LSZJ2OICnDy0MR5eJdIbWbmcp/y0u6ABSEltNt/87XXUpiY3Noem1nkvF32MjNPvcFF9W51wFNMfTUoZqdqP3r7NytN3os1cXu+Uf6aEdBNX1eicX85Mk3keVOzTHNFaDnuK6GI8TfNHGLQ49wcrFvgoxv7l1U82muk6jjzerYz4ATaq1cZxmu67nqccNYdAJG6swoo25+Mzz03m6uThn6IzfzIE+s/4bttOrKaWPpARoum6llHLWHTuCkkD09xMZPFViw+sQUXv+jHgQJppq2YvH63Hvh5SZs9J2afa0DK47jXkOGl9LZ+iArz+J+Bm1l/F3RrWvlIvPcm2oTyo0vt4rSdwPMsf/fpnHbhme8rBmn9qPNOCBx/i64FeIdHjlfW2ndTTn2E6H6UhozAVrh3h2ztIJ77POAswlhpBgu9czuWA2+aIBhMAmA49HQfuki53+WPsb3m5mF7Tp5c74e/R0Pt6ukFS4VzpjHLlnYh53yb+19TxwIb8HGtOgzny8E3q10gD+rL5k/DuS7DGI9xTyMU+dnnU/7mHKG1M1OyAL/U6/t6jsjrglzWYhpsTNBjzGjKkRxk+9EQn4O0nXnnE/n43q70UcBpdNPD09ZfFmxG4wf4W+1OAb6MjT+vI0jJRJvtB693+cp9l0UohpkPYY5qg6R/B8ReFxrE73Cfbn2zul9oOYA1NWtGTnApbjvYW/izgEq9/GmKsVweO0dB+B+CN5TbGPO/7KAlau7St4H35VeK7QPVyjT2nwMj6rpCH6mD69d8PnsVfo9zee7+uliMPydiqPtdbOFfFRZr2DWDCPPvQ3K3f4OvYaUyajL7sqjHu/RN5Hgw+QeHI8fMUdk/kqxsTZSfzd0tTLe+IwTxmq8B2i/ps8BgyNKzWThP5I4F1C3ZyM97SbxE+I2Rmv2f3ceey39xZg7Sl5Dn160HoeuyVWYSzTODL138C96mNPjL+E57tZgv6mT71KiXoJ/Y1er+kBHnvCa1Wa3Wv8G/JaYIYOacDjrGxpulKzO0ybptk+OxBrol4ub7vkH9BIpgTEKksb/ZXi4Hsej32BEjR1u1JKjWyHd7V5Pk/pTYl9At/LFmfs0eyuqxHHIvsBHsPJ1A1js6EJz2fO498JKK5bfTW7Kht9MXLGOVYud6rj1zn/vYiZV6mL7Rm+Fcdn38Vn9woj8WC68DhahnrkHi8gVtGJe3mO+q4TEF/FdRrWq1nHsD555vFYOkuTMX7C30Z/sWyvx8r16xih2Vuy1ml2o4Wof24IX+MiNyFWUZexYzV752Ieu80zAXvti1sxb275CM+gj3X35rKz6nYQDxRBEARBEARBEARBEIQakB9QBEEQBEEQBEEQBEEQauDWJTx3McUudUfKt7izc1UXL+mL3zn6lJvkmWg6zwsfQh5xZsBcVmVdMdLXZX0J10CvfJ4i6/8K1M3aWlnBT1r0yYdt1Hfh7sdGOzoqLyN3m6RyDyaL8UfqsH+4ypFUylX34N065XG3WioxovKZlLdwr7vbzWJ1aBrjgm/gsuyTzNuNpgs220kVrJc13Wpq5tvFe6X9PlU4HPIen/OQOFU9gHRupr+OsTruV9D/XQwY+lMb/MXKrU6Fi7RTCVyVrURmkt+du3d7rcK9VgbCVfAfv9C6oD9SaVbuYEiFUoZy18+0SqQt2/Z+J832rZvJylVdgmueoSXcYa3Hz+DvRJKklFKWWjzNsiMI+Bpup0Gr/dg5C5Et0dSv/iTldJk/lz1UeKKPv34F545ncXlQ3Y4oZ9yFlHcBB+FafG0cT4drIlOCTxruod579lPG3xwC12eXAoxuF0MCK3c8BVKsuJn23f+tQUjbSWU7dGzrxxiVKDmKoP3o31fvS+An2yPtpqVjPO5rj64cocEMuEdP+Apu/sdar+IFybyXuBDjN24cl0RRygIwlt1JGvWCUTwVZtRqkn7dHZ8T9NIFzV4dtZ3VaTZ7omaX+2GNtH7CiqmoGSQF5zDb+VFzm/M11/uct81y/5bwIUg7qk/jGvEa+rKxRSPNrrqI+aNgJL9/1+vo1zcbYZA0WFjOypkykR6RLo3mZLjfR87nqTmtDaM0u6iBj2Z7ZnA5Xd0duGJeU4z7svcxb0b+Po7VSRuwSLOb7oDrdUB4LVaOzkqLn/xSsz9YjLTAmSO5GzqTU9S8dbhtYkKusWNPkja7qBNkT0zCW8blKinDsK6F7sBNdntsDCvnfwj7E/ooplJcz6ST59H+cm0W6cfHfVg5SxHkAL83xl6h6V64+VPJjp4692JAr/mpMzsXvgF7pnbfQbbz+kKsv0Nb1GZ1zn0Qbfez7pRfxn5GjvieffdfkH9G/wxprqESY6rPmRusTqzLUs3u7Y4xNmpfF1Yu6TBaq9XbkGEdfR37vRtm/l2Fpq+ls4998aVSwYds//1AGZdO0LSsxrewx7y4mktgKyvQL+uuwlim8pUwdUb9N4h9Fu/DFBvFzvW8Z5pm+2zAnGou1knICVs7zdPsaCLrb/HJRFau2VDIIw7swxiOVrgfOn8ppVTfJg9odug6zKnlXK3BUsWPSYJMxBCKPc2+jl+yOiEP4l6p5OqRc3wemvvdQM22bMXfK4PQF64+Wqgo76bRNXWOcgR5HTBmjqbycxk9ydq+g5wg33FLZ+azOld2YQScmbKGnOG7dtc/MBiujMH6efYBtNXATx5jdSyzsb8wd8GexsnAJWPnp4ZpdruEwZp9bgJkgWMzO7A6jd+8gnIvYw+sl81lP4v9p5l8jaXysdjlfD98u4gHiiAIgiAIgiAIgiAIQg3IDyiCIAiCIAiCIAiCIAg1cOsSnv8SM/cOZsdxlqN2Sv57aGaF7Bfg6nWuDVzSup4axup49obvlJe6RdnOrWYw0kuMHICpDiRHVZez+MlIEhHZjlxFOXFX05Ig3OPmEvhFPbN9LCsXp+y4nxN3dUMrHnVcnYDbl9Nh2EY/X1askshUksfgff7YBi5lg0+PZnX8J8E11ycN7UZdupVSynIqUdWEOZdH/neqJhPHv4G6Klf5ubFz1C2V9ijaWjdGc/c0Ayl4tALuh9fNPDo3xeoDN2prHTh+e6Vxl06TL9yYnS7i/Zh1EjCacSRpHqQgqY9AttPm2FBaRfm9j2f33I/nLhrII7a7EQkPle2YAhEZ3qIbY4Z9J5SjoXIGvwN8zJnTMzSbZi5JXIBnadSIS5MyN0Zo9ieh8M9seZ73XeMeSLaoVCllMNow8iUupaEyItMBuNimfsD7zpj+23CvxXi3AS6QJ1yp4lKFBl9iTq0ibpwuF/n4sSeBy+oH986Qzfy3flOu7exB/4a8QXBFNzbl8gVrOeQU1mN4T+ZqJHOmlXA3t66CK/+nUdylPvErItsZjyxaeU+jDQKOcxfhsgBc26s+3OT9p/CMZdd6YI6fMuNnzW7ugrHyXm4LVsflBiaK0E/sS69oH/P66YDNv19rzTNKWeytM/8S2g6FDbjO1EgyfNAMSjTLWZU7nxcyW2Emvbch1oRz7RuycsFz4OY+IwX2M6shrYl8mb/D4vvhNp/1ANqx8Tvcdfx6Y2Ty2TQF+qlBp57U7NCIXFan7fEhmh1DMvcY9vN7yBuLvjXjJbyHrDdwP3Gj7OgYHIipSQPNvlTIx3jIQEheLr2CNqz7gX15YdqAX3AwAGaLQ8NZuTqfQyL856plmh21Df3oz85rWJ2Jj0KS8EWLHzX780f4PubKOszLRZchGslegz7V2JPP3fG98ayZ6dizxX3In/XCt5AqWNtg+95+DfqEV1cuk/MK4nOHI5g4HhmA/lrGs2S0vh9jfOUTkAd3ODlIs2ft68nqdGmGOr3Dd2t2Q89sVi7n2Xyb9zP0OMab4YR+D1eubDEukesgHvHEe6JyACrZfOciXxeyZkRo9rlxkB00XsDlKzFEEkuzTSV/naDZffrwPlr8CZfF3w0yH+J7157DMY9fnAwp2JrovZrdewDPOvXMcKwfdD8V4sK/u107SGQ7ZB+39hLmmT4xXVidit+xdzF3xt6peDOXHnn0xmdt+SFBs3t1i4f9/AxWJ2SW/exKlEU5WA/zG8OOnXrQVnGllFKP/gV52cXRdovdFtcmYw4cvpXvfeOILEtth6ylpBh7bGcjl8/UJXNL83L01zrHeL/blvUNOUrQrF6hyGq0JesnVqfB7sc1uzKbyEdP8Sw8pjr4rNrT8G6bPIL7OTOFZ/mqPIh17cEwzIe/XOLfx4d2xT7LnEiy5I6HGfdlhqJ02wqp51/bVI2IB4ogCIIgCIIgCIIgCEINyA8ogiAIgiAIgiAIgiAINeAYCY89iQqRayirLmw7qeJsQLnPOvzMii1S3FXr35DxJndLXzP6c82OJFKVVgfhDhk2qJrI2PT5LDw69y3Ldih3IdPRP2Q7BMuZCzb/bmiDCOJVh0+xc8434YJFo6XP6bqClZuvkKXFFEdc1gvgYm4s4q5iZpKhxGrG+7zwMY9oPqPbes1+yBPP0C8B7leBA/iz2UkepAyXudv0P9rxFqi6kl1zoTshDW72xpPcNT9rBtz5Qj+x7dIcsD+HHRe0gGuwvxFt18mdu7xuU3CLM19I1mwqhXEy8d9eq/IhI1DETprDs1ss6Ad36d7uCZq9oQSZYgJHcVfD9Mlwka5HPNHd1nEX88qerTXbSoZfrS2QRagc/k5MfjxLjiOgcoaCwfeycx5EwpPxJtowbiLaUN8DXcYiG02/0yM1O7X7N6xcLxWv2d6zEak86jm8jNIHuesnlRFZ7oMrbpUHn4sauOJ6M/2RZWJYWlfNfnzKZFbHeDRBs53jG2t2WVQAK+dEJDxO9eB+GvIz+p75Kh+nKZ/bzvrybzj3Mt5zde65FL1sh1LeGfPC8azNmp2hkzotLOmBA7IGBG2BlOv8B4G0iqqVivHnS7KVpM7iS/q5Djyj1X9oefgJza7zEJfV1InBfWdNQR8tvo/LpkJWwTW3ogHaI6cP5hbfvx0vS7UFbYe4v6opSKgYjowmAf35HOhLJEGvD9qg2aN78L67ZWaCZvcYOlqzI/dgogo94KUoVyah/YP24Jx+HcltiywCpysgAWvmj7F4pQ/P2JU9DG7yetkOJa8tVkSvQNxPHJHN/De40Rzz728tP2PnJqiONuvkkkxi1+P5bBm9Gv0wZehCze5ej+8HNt2Pcr0Gwd3cOBX71AmjprA6xgPIbPbMfsgY3GZyeW3xVciFl/SGvOXFL+A7HrmC78vy3kWGDSp/TlkZz8pZC7DnDNuDNdPazn67eV8gsrMBdovdFnRNjdzA5duh9SDRbE6kUyUpkPn6xuSzOrt3Q/YX6Y37TevP06203QCJ2qGW+K4QtRbr7OTOfI3/7aXumu1xOF2z31rCs3U9Mg1SgdSP0MfMnugTHvm+rE7tsxhHVPazNYOnLOupIB9pFwkJKK2TNI9Lr2oV3P0IC6ee5/IIJl3qhL1+r93xpNRpRblC9qIHV2NP9nBdvtfIjYdEduFK9PFWS6Zp9uQjf7A6fzRHBjS6IyncyqVHjQ5iz9jyMMIuFP8ICXnkcPvSv0YLIRn56omF7JyFTLE7h2COSh6I9npn/JO0iop7iuw5R9v92Nuizpe4/2B96AFib2mE70fR9yMTW0zPdFaHypa6jyRS5J18T0P3K+Ekmynd27d+nX+3vvAu9h19+o7QbGMBl/5T+baBhEKg8qJmbUewOv6LEXLAhcyVj9Tle8LKnlhb0ifhXOP56K/1c/mzbj+0gRy9pGpCPFAEQRAEQRAEQRAEQRBqQH5AEQRBEARBEARBEARBqAH5AUUQBEEQBEEQBEEQBKEG7q7IjsY90cf3IIcWouDq6sZ1wNO/Q1rj6MUo55wDXda1+7k+3G/YJc2eFA5BdB93nlr3t2Jorkaf7qXZAYvdlV1obJPq4mXcSjwTfdriuxADhWKK5fFkDCUkBokRv6Xp455Q3PIsNv/ez53r+l8g2l3PfXifIauQKi73vjqsjus8XDveH2040vMXVi6iFlI3DjoDLb95O3TiRnfehpYS22lP9SmJKU71oQGtrItrG/Ym8HJREXav8W+wNiTXPXqWnaNxT0xBeI8sVsQVHjfCcg/0+9HOnsTmn5v4FUmp+0aaZuf0R+rHwkhWRfncAz3oK3GbNHugR4KyR/R26EZj5pEoNfm8/0V+j1gQNJYN1cIrpVTAItua/zSiZ9an8TXfuKEv7lBMZXxMW9sjzkitW8wuGbQROtHxM3Zq9s5S/vt30nfQsTr/Be1vTA5i6RTcx+MxXCaxRJwLcL3UIVwbPSq9i2Z/moL0hsZFmHvd9/C4ISwN73dIM1jVO56VcyLzYNVlxHcw1kLHvPQy16tHv0D0zM8ph1DvT9JWbXncJXUIfdLoitgDljL7qSYfPIO+RdPLUu2+UkrFPIeYOdnTSLrWtej3/rX5/HqoG2IJFYwp1eznL/VQ9mjxKUmt2Rn3ljeGjyOPa1jXQrchnoB5XpKyhweZb31WYK6t7N6KlaMpQR1JwUiSOvxEPjt35QH019oXoKkP6H9E2SNtDPrCo1+9oNlzxn3NyvV+6DHNNh5O0GxLZ6Qq3bPTlVZRP62Zo9mtXBBH5rXpvM+VX8P8PXHjaDzDKYzTgCK+p/FNwvPRtJdUP6+UUt5nMbaCv7i1uCdOEeE1F7pNvH9E3x/iOZ2du/E9xtbwZkhtezgecUCMT/K+W/sMxkm/L/prtuE7HgXNNxl7jex2WAsD/RBLIehjvje49B7SdjqlY+t8+lk+V7b4GOPso5WIr+LrirYpjfJndZxT05UtokcksGNTY8SWW7ptj2bT+Fe03ZVSKvC441O+X52Kz2j8XiY7V3URxzSJeeZrqFNwyUdRUkfajtXUt3Fndjz3GNKlRq+epNlG0rwBTnxh3bVokc1rr77J16uP82I1O+werEPO7yOWgnEX34fROBJRPyPexOgn7mHl3J7HnjWnfb5m01Tq4Rv594lrrclaPVg5DJryvcvTbdg5GlMiswdZ5yY11+xd7Xl/H02mhW6ZSG+d92MpKxdZB/3izYexbw9PwNz0bWs+nkeeQEyV9u5Yfya+x8vtycB3m3Mdvtfslh9gLCYu5s8aNxbPGv4O7mH6pfGsXDEJ4fjoiy9qtudqzF3Oiqds9vibfyd1BFkvYvxE9ucpuJ8KRby41m8ghbLPgOvqVqAxxOgco5RSYx7DWrb6h/manToEsWJ6PRvP6sTFknsgQ+Hweyvt3sNPRRhnyxogHl3ow2dtFVdKKZX8A+4tJoTHPkq7gHXS7xT2mCFbEH9qQypPfXyFxHsJs/upQDxQBEEQBEEQBEEQBEEQakB+QBEEQRAEQRAEQRAEQaiBuyzhsS9JMZBTZlLO2cB/0znXHS6zezrCpex8eahmP+mdwurQtMhUHvRcVidWbsveeM2OmcZdeexCn+lOUhX/L2JO4m5f9P4tHSAtcHaFW7Eq5K7jhfVt/+Zm0rXbhg5w9VrfHK7JlePRNh09trA6HVxtX/vxi/ez4yMbkfKu3rtwvasdBpfBKp1kx9QI7pnqGlxzDT48dRxNq0VdUQ0XuZsqq2PH/fbfYiVSKoMTH6pUHadP8aoRwiVSJXVu7ffSww9+odk9TsGt+vhrcN2k7nZKKTXMy7YUZnp2S3a870PIgxoehpus5RpcXPUisczBkFLV/Q7tGrSDy/2sJFWuJQFuf7Hz0KbJOtfPiNfsp/q8U3Im4DO8LnG3XKcL6Ee1veEbanCB5EY1jqFVlLUMrt8DPPD8+jZI7cHTGv+HwV2QxjEhmrvfriiCK/lDHpfJGS47cDMhNadnb8wjBhfUKRrCUzb7L8G7pbOj6x+69NNE5mGqJK2/C2lDaVq7u4X+vig0nWjABrwbnxX2141fpvfUbGNtzHuVutSrv1zCNTyNCZq9dDwkpmN8eF+feRX3c+IGnE2tXS+zcopkSw3+Au/wkiuRCm3k60LKBLhDRydgTmV9VCmV+Dnuof4feCaa4tTt9CVaRRV0iFB3A58f4Y6tn9OD5iFNM5V2VhGZTW4TnopWKTz31+O/1Oy30h5ipRp9hWvPDU3Q7IZLMAcYI3nqxp9uYJw0qgMZwXt1dNJZcnzv93CJ9v0e48rcic+vpYGQ5hTEYiwFdohn5WhfSPsJ7vmRw04qe1SRFOx3A//F+3XHsA92Ror6iz+RvpbAZdBr312q2X3PDtXsvPX1WLmyVpiRoltjfVgSs0qz6zp5sjpqyc5q7h6494YMyDgHe9P0b/AMUSt4ncQFZF2cCUlVztCmrFzwY+ma3eZV9AnP3tCw6OVad4OgufiMLJ1k6PgrSEXb9wFoT0L3QJJVOL2I1aGpUp8YB01m3rd8/9nOFfOoRyb2Mydf4OsaJeYvSIWTH4D08fXVPD0q3Qu4qHTNNjhhDruyjqeP7YWvIcq6GP0y7Sn+fSByFvpSIukHcU9h7tdLr5wcr7xSSimVH415nK7RSnFJ0uwb+Zq9qYmvZo/WpRdP+QwypABUV0vu/Y6Vi3JCW5b8hnE7pX4HzT58z+pq7hxzW35DfsZ3M95vryHxml1HoZ8OOFFJq6i1z0MedvJF9J/e4XyvXfAD9pWmw1j7jR5Ip3v9keaszopImpLdft+8HfqPhGTvx4M8Ze/0/ZCSRpE2TWmIci0OcPn278VYC5O/xzrSJZZLdQsq8cx+JtShKa8LRvH7SXwckrwCC9bSXqF8/037G5XtPHIO32nWjOGyZNcPScpqI/bTp3bFsnJD++E9PNATc+oXX2EMR67nKdhndtyo2RNVzYgHiiAIgiAIgiAIgiAIQg3IDyiCIAiCIAiCIAiCIAg1YLBab0170sM4pOZC1WE0sUNTLNJ4xK1M1+wPguy7H1JpDqXSyl2ij1fABWvULrjoxD3JIyXfEtVlyrlVCc8dSH3+tPxcc6FbgLYbzdaiFJd+WO+DhMew/4Td6xmcIe956swFzY525hGQaXYBeyRWctfmUBPat9mmKZrdcAp3baZd1hQACULV5Sz1bzHFQVZhToT7ralJA/ydPLceR7WbUkr1DiJOZFU8iwDNHkMzuxj2oe0MLZuwOsY0uKI+sh/PUGLhrvnDvSF/eSu7q2Z/Gcaj1lM+yMX7+fYsXNT1LuFFj8LVz2sV3FftPYNSSjkFB2l2VfZVdbdwVNs1ehUSqOqkJznPwJ3RPQfu9uVe/HftwLVoj4H74V45zof39xILpD4lVrirBpg8lD2azkEfK45EH4uboJOzkPn7xmNwN699GhkPrEfPsCpUmuNyMFGzL3zUmJWLnYR+Ze6CkO0uaSQDTB1fVofK2xzVbn3qPavZ1c0lNHOCSwHWnnJvvj71fxkZ4O71QJT8iT+OY+UuPAV311wz5sSl+fGaPdLnOK3C5AUdTw7SbM8HuczCGAc5jtUZ92eoJGtmDpffXXocrrBBh+B+63SUz3v2MptRuWFJP56NIu9xPN+5h9+yWf9OoOtc0nL+mandIW3rMgb7AdccPNul7lz2YyqHPXXcr5qtl1JFrUHGhtTBkBs/mQG58Gshm1idvaURmv24N6SLfaK4G7S5FfFTJ2senR/1mSUazUA/uzoU9fVygOtEGRI18/ZljI4ac713YcwlHazPztXfjEZwzof8g8ozy/vy53dPgjw3qw8kcM9P5NKAIzex//R3Rp/8di/kCRv7zmZ1GtWC+3o5mV8HhPF7oHup4FnIYpc33Fez+2/ke9G1jZG1o6ob5s20QVxOQOdKyuWZkH+EfWx/zXFUu5VeibB7jr4P6qJPXf7/cb2HsKaM+mi9Zl+p8GXl9uViPtvSCOUeeArjOl+XVvD4q5BQNJmH9a7KU5chj2zTI1/BmCgaRvYs6XwgGUvRDywnIBOgz60Uf/aUT7Hut20PCWBeB/sZAR25r/w5Gf1rEVkflFIqeTaeNZFk4oufPVmzb0bzvajzdawrMfdBDhflybNYnXob4yJjEPY7run4rnBuApe79LsH2VE3HOOSfwqVigc4Qyr051TMw5fv57Lk8LcxTpzqQgY7d+8qVm5ifcwJiUsgv9rdc7Zmj+8wjNW5NAipiU594Zg0gZZsrMvdRzzFzm1biTUuausYzQ77HfPHu58uZnWe/wxrV6UHOn/oZ7cmAaR9pVnLNHYuZz7m151fIJxDu2PDWbmjrTAvt3wPY3PNzE80e2r7oazO2VfranbaQGTYqm5+oeOxb0OEhDAX8oxddB7as5ZnhbOFeKAIgiAIgiAIgiAIgiDUgPyAIgiCIAiCIAiCIAiCUAPyA4ogCIIgCIIgCIIgCEIN/PdioFSDU1SEZp+fEszOnRoyV7PtxUCJ2zCBHTf4CjpF6/Ez+uK3R3UxUO4ijtI89vIardmWYh5zhKbh0p/TypC0j0px3bspFvrJa12CWLknn4M+tZVrumYnlEEb+PHB3qxO1PewXbJJmjudRt+cw+Ot3AomXx/NtoaH4ESyLjWjvr3/82cSn8XgzlNeWgpxr1tu8tRt/4bejV7GZ1bwFGwWb7SLoRj6cOtVkg64iKcKvP4ktLd+F9CO6QN4G/fujjSk3X0wfuZcRDrcSwfDWJ2IV23r6C+9zFPzhe1CP3PKRwwC81nEyDA1juMXIc9uTuZay1uBxskx5+bZLeeoMddu+OeaTeO8KKVYLJGSgdDTem45rdk3Hm7GqnheQmwT085jmp00n6cNds3Ctc9OgpY4btcTmh1fj6eVPbkdsWui5iHGhTU4kJWznD6vbFHWH5pR15wyds4pHfFqKmKR79G4m8fzMLQmARlOoB9YWiPdnD4uE40zsG3vazbv7XbpEwqdt6UOTxFtOUlS4TZFbAn6XoyuXGOd/TRicfQZgxSEG7/haSALWyHWwxvtkAJ0tDdiVDWbzZPqVfhgHbI39pTi8UhoDCz/NdCJX3qdp/8rC0AMAa+f7KdpvjYJY7vOfOimacwA1+tcL28w477/+vMlu9e+XVisr5hIds5gwWd+suNHzR624AXNDv3Evu77mSTEFfkmi7ddztcRmr3/s4Wa3Xg+2ssvkcdoy74Pa4ypHHbky/bbka7Vqa8gZaa+7a9NJG2yAM+U+DWP0xE3/rCyhakR+oL5XJLNMko5bq4cuh86/OOX6rJzBgPazWU/UnDSFMw09odS9uM4dT3F9zcDvDCf9N0xVbNbxaZrdsnDfK+X0x/rUu1leO+z0nkbvDgAaXOf/XWtZs9YgNgE7lctrA5Nh073w1YT/x+nlcSWy+pRW7PpO0lc1orVSeuF1M7GYPttejvQ8Vbeh/etjOHo7zT+EI0X1HA+X4dLojHful7DviDrfh6biKab7demr2bTmFWpn/BUqdGtscf7PGqNZs/oyuMxpD6ONcpElrKwj+zPDVenov95ZeK5XfL5fi0/iqQOXor+cvklErumms9xZAyU7h3f02znLL63pqnKxyUivf3S1vEo5Mzj8pjzrmt25qt4noAzfO7/bs4szS6y4hrNa2HdbHGIt4mLM67h9y723dn38hTjwXPw7pLmYl8UOxUxg6qLS2PpCNu4h5ej9e59iaSTX77fZhmllOo+EjFKdmx/WTmC6r6D0/dO9359OyM+mjkpVdlDf/+UjlMwbq8+gr1K1EjMoXlP8fhdtI/njsN4DBvJ9+8Xf8V3yNDFuAf6PdPgwmM0jjiB2JQrGmLNyBvDx71POvbNmWMxHt33oe+UduBp0iNHYT+3tVyXa94G4oEiCIIgCIIgCIIgCIJQA/IDiiAIgiAIgiAIgiAIQg041Vzk7lOVmq7ZMc+ls3MPP9dW1USc4u6oVjsyjH9grxyV6Tg4PfF/G3vSHP05ezIHgwdPgeoUSNIGE5cwf5172O+LUe535a9s0chb507qBlc+mmLZFMjlBOx+wkLJASQMVRczecFQSIyoOz5NX6aUUhZ/uIzStHQMXeqru4X5AlzHnerp3JvTLuN2ekLq4LEGLnJUYqWUUmaiMDCWwzUy+AB3Mb/wKyQKJ/3jce3jcO+MyOZuy1SGYT0COUp1aXzpp7JUgdXIBozNSWpPI//9l6a3NDWIweeQ9/gPSVpZuXI0VW6YF24O5a6NpbVxLnAh3iF16Pb5gT8/ler4NIWrprdOVeObDDfFPg2Qvi+yCK6WBbp79VoPyVdlE8jrTCXc/fbaZHxuyCqMW4/9cKfUy6PoFYwk/bRe6qKq8PSXp0LWFPI5+s4/2q2aVOt3DJHmGQv4vJlD3EP9kuHfnTIH7Wt14+Mobhzuf0c+3l/tbN7nQnbDZX1VCiRVp3ZizFcnMaFjx+ccb2HrWbRP0mSkiS36CPfqeSqd1XEmc291+CVV2Px7fjTGZb2fjrBzls4t9cUdwupLGEtD69ov9+COKZrtf8P+mn3pF6SA/+xljD+PNTyNrHtXrB/dRkGiUW8H2ivzNS4zCdmHd+9zEJK6iy/p5CixeL9xY/AeQ//m8gBK6AasexWd8K5d/Li8LuMNfFb9jVjPzGTu1qNfKx1BA0/MC1k/xrBzbusgwWHpcL+I12x9yt7Ehbb3i9+v6MGOD/aNwD3Mx/grOor5kEpelVLKPQftVjAKY+75CFZM3Rzqq9lfxGBtDlH2x3Dah/isTg8gRXv2iABWLvEJSF3iluFez5HnNhbwdTH2B8gOUl60ewu3Rf7jZD48y6XCr7bZptlUJhGrMHYsOrd8F7JGW4nMtWgsT0m+sxTPVhkBSaKBSHjCN/N5qWwX9ojPb8B9X54ZysrV24o5n6UKXwSJUsxyPvaC5qJN6Tu5MoRLtNyPYN2n8tPiONtz6N2EPtt9J/m8sPUNpHj94HM8T+XyfM0O+YB/bbz4LNYsn2TMqZe78H448aFxmr1p40rNHpiEVMV1n+GSovMzIzS7vB2u51TC5+7Z6WiH8dMwFkoGYe7+9Drvpw+fRSiAvp5favbIaS+wcr1IN8n5Fu11o9F9NssopVRlf8d/tb7yPObs4ntK2Tnfv/E+6Jjz2YP9QAFXn6obG6h0N8FmfaWU8iDjNmx8BE6Q77wG3t3ZfN1wMe67vHM2KzfuLEIE/D7H9vfE0p4t2PG2G5g7fPaQlOUd+XcS2r40TTxjLj9M+v729ifigSIIgiAIgiAIgiAIglAD8gOKIAiCIAiCIAiCIAhCDdyZn5EjMtPcqvzlTmQytFx19f+t7OZuPreDMLSEK7I+IxHNykAlM9RV1xzoyy94Gu77NDqy0YtHxa4u24lWRi+FIcc0e0RVHHcdNpAsPBYSAby6hFKWlIuabfKDG2zVpcusHB0QOq80YOTZoEyBtl3P/i1OIchIVRbHsxw5bYfrN3UrN3ohW4E+6nYgOaZvyu1oNTdBIuxXERkGzdagFHf9tifnUYq7PlOpSnWynawX4QIY+hlcNZl8S/H2orIdSnGPpuzYfWOC3c+9U/x/PqnZxjrcBdsz7aK++P9A5gUaEV4ppWKnEBmEhctEKDkT4FJaVhvjnmZ40M/dtR9E1puSh+Hu6r6LZ8qpQ1SSVtLHjD6QvNG2VYq3b8ZbaEMDVwepeu+hTUMS8PekL3E/oX/xOj7HuCuoI6BSUr38zS+RyHaGIBNG7FT7/Va1RTalghj8r8L3DHebNnvgepW/IrPGnjnoq94ducsuzRRQHIJrV7n4snK5T8EFnmaWoC635luU7BRv5u/E6xFIHOnYi1iCsfeP3mq+O2veQ+OQScVFL+ltDxfguKcwluj8quuSqu4jWCv1/ZqSMxXZAkIG4n3QzCS0fyul1KVXMBbyGkM2F/6OfYkH5UYc+kvwTi6FuPAB5pvokRjD9Xfza9C2TPLDPsAvHnNI4Co+d1+YVu+W7u92+OuNDnbPBe3H3ELdyvucydfsPx/k7t008w6l3twEdlyrP2SEN13hBk5nxwJdMriA1egTrkT+nPEzz5oWPgRzQs7vkDcEDrig7EEzMJ14Gm3gFcd7ZvR0IvtshTl+X19kOPmjmN/4h/v6Kkfjlof7Ml3nmSw+ONpHs30m4D3XGQoJcNrecFbH7I554bHuf2t27nz+FWVq0KOaXTwBrRWzF2WmL/qB1flw2hPKFuFrr7LjCRs2afbXXbtqdtw4zCfJX/C5oL4rMh5VeJL7+ZxLfcyemL+3/oJMjR2njlf20PcrR0GzFG0J+IqdW/4o1tymYZBFXVkYjUKH+JpXnwy55Nl4PzHTeDm6RtDxnDQ3QrNjr3CJZMrQzZrd41x/zf6z0R+sXOTv0zTb90l8jwidCPvnz3qyOoV90W9/fxrPd/Mxvr/3JLJx1yTMt2UNsKBWdeWZrwJn2M94c6eEzCJ73wg+foqb4PsBkzuS99zgCJG7KKUutMZ3ucf3Q7ql2tZn5S6/hJY73RSZaXopXPveicdoFdW7/0jNXvvLZ5rdaCyXYlP6XsT6WUlm4t5/8XfrPhb7fgP5zrclazsrR5+dSjvpGpE7nss0Yx4jMqCRqkbEA0UQBEEQBEEQBEEQBKEG5AcUQRAEQRAEQRAEQRCEGrgzCY8jpCd3Isdx9Of8t+Q0/4vZephsRy+9KredgcTqChdhKtlRSilrJaJQ02wa1pv2s/1Q7GVH0WNoCjdUw94E++Vq4V6N/kSao5NKWMmzGkLhuq1u8KjfVNJDJUq0vsmTZyYy3GrWp9uk6gpkCk5X7EsWaBYOv72Q9liKeNTxsgfhxlarCC6mBRE8K0rgDpLBaBPcV6u6EVe67fZ1P9SlV+8aT2UdhjZwUbUeRuYBg3MtVofKdijmkNr8D1eIS64dqYvbb9zF26iTajiCrKfhVh48h9978SMko8cvxF2VzAumQ2dpFWW18yx5Y7j7YfBfOeQk+rWFSKqMiRm0CpPRua/l7rP2MJBxT8eLsYq7+GdPg1QhaiGywVgC/Fg5muGmzm7ISWIn27+f5Pfvs3vOEZRG8b5V7oulksl2tpOUL90uKUqlL+aP8LfQDyztuexAWdD2V3dBrlhvOerkPa173oY4Dt6LcZ4xnReLHYJx5RQMN1/zjXzNLu/dhlZR932I977hW4TuD+7N+zJ1yS7vi2u4bMScYfL1YXXyGripuwH9TD1JT8N1OSzUdkYdoy7bXMW9cNu+0QjzO38aLtuhmaJcc+Cyf2Uaz67jno32rrPzimZbA7gUNPlL9IXIYZAFBs1DO2S8zq8dPfLWZECe/dFXYyrhbk7HbM5QLnesv5FkDHn+lj6mRryOQSbgtpJL20JcMTdRscWmJr7kyI4kUkfoDv6/wuxhkBxv3fetZrd5FRlrEp/g8oZeL8drtqkJpDnhZIwpxaWUgQPgEt70KO7ht508W1CD2WgP/yU8mwSleDD675WBaI9ec2ZoduguLo1O+30JOZpp99q3Q2E9zIdrF65k536+iT3eT99CzpOYifkn9nX+jJdJBqpNn0JO4HONS2Eun/bV7JjXyDXIHuxgMZGbKKUMZH6l0kxzYgor9/Erj2n2qr2fa/aYcMyBMc/Zl2wWd0W7D5nC5QSfnIR8JG45+ljsQbLXqs/Xz9/aLiRHb9j93Nslch1kh71mxLNzlUswV1LZzuQ3f9bsRYWPsDpXOkDyMqH7Vs2ensXfb+s38Ny0jzd8l8hddOtFlzFjNdtI5J89i0ezcmHBaH+PX5CecAORs0y70ppWUcfegbzVEID13r0/32tb5uP+zm7G9fre/zDqm7ngP9RNn+/QsZQt4ccu3W2vfzQTTb1aPKxC9yTMR/Nj8X0rcSmXhXrtIftxO2rWTXt49pqwungfoz5AVqOARXzcxx7GZ114vrFm10oj+8BL/LvGJjsSJX32oJtEphrXG/v+B89gb7x0If/uRiVot4J4oAiCIAiCIAiCIAiCINSA/IAiCIIgCIIgCIIgCIJQA/IDiiAIgiAIgiAIgiAIQg3cegyU/8U4HneNW0137MjP+d9Edx8GN6JHz4dmz+ILPbipKphWUVXpiKFgKSOa5ba6lGtHSBwHEsOhurgnNDWuOQH1qbZcKaUsJdBwmmmcD31aZIK5C/SOaidSbpm8vXk5cg0jiYFS1boRyuhjslTzuXcN8r5pCmAac4TGqFFKKbfLiFNDY+P47eKXNpMU0k6RJKUZiXtC4ykoxVMcV5cO1pqFcjfi0M98qITTyvWktF9Y06AZNrvw6cu5PuJR6GPg2MPiYz+t2p0ScJrE22mtS5ucVaYv/j+0a66Z6X14SnAXZOpWIQuQhtV/KdeTFveHrt5QhTgjLjdwP/pU3/bi0OihcWnMJI04xec8HwdeP2EMF/dDjAz3I7xt/JdCs1zWEzplZ2RYZqnHlVLKucjxcYcKh0P/WhjJ/7fgTDJ1+tUj/UwX94RS5of+aSFpbd0u89hElpN4ft9w2xpcvwu834xe8rtmr+gQr9nhQ+ynj7cU4SHMbYneeDPXTx/djGcPaYl14SaJv6CUUlkPwI6dZDteTUU8j0fA+uxiu7d62xh2IF6ItStPTd/4dcTZsJI1y0DmNosPj4FysS9iAUS/iHtOXMBjV4TtINp7ElPFRGINBR/h4yLpW8SSqv1NOuro+jjdktBYMi8dw4Q9ZR6PgZK1Fu0a+jDG343RPIZOcSgu7puM+TZkD+41eTifh4xVPDaVI7DkYXIr6lTCzq1ehvfk/jLW4lrkddaZz2O+0HSiTjuwXl1qx1PtKoVjqp2vrdDWvZbFK0r2s3jX+thWlMIYzLHHiF6/xScTNdvamqclNwegfU2e2Je9tZHHF3nrHvQR17xI1PkL96PfbUavmqDZac/ave3bou7wNM3u/coL7Jzv93iHbsGYHxuR8Cv62GjFMYjnEvYR1rgZKXxN+rw/4m9cHYc+HbQ7V7MPjORpaK++iTEfPgTxLHL/4OmeC89h3ht3z0PkjP05le6PIl/Bc//yw/2snPk1XDv5ccTWiQp9SrNTu3/D6oy/1F2zFzswg7hh3wnNTpqjW28qMRd4/4R1Ye0ziHGR8SCv0vhdrOfflPfS7OnjFvBLkzTPdN9hJvuO/Mf5PPXGG8s0e+YSvKviBryX7+iKmDULXu2k2TTuSn4DVkXN+RzXnhuDmFcFO/mc6tUGa8N9J7CeNV+JdSW9LR/PF2i4Fb6dvWMMJG25U/cz7Fw52V80WIZ3GPEqXbv6sTqeqdifFM/BTab1WcjK9RoTr9n35uJ99j6BdONbAnV1nkOdGzPwPq9N4u92ku+3mn2kPvpYaSuMq6BDAfzayGLMxt/Zl+uwckHfYx6o+BPfQdfjNaqi73m7ObvoZ6bqEQ8UQRAEQRAEQRAEQRCEGpAfUARBEARBEARBEARBEGrAYNX7dAuCIAiCIAiCIAiCIAgM8UARBEEQBEEQBEEQBEGoAfkBRRAEQRAEQRAEQRAEoQbkBxRBEARBEARBEARBEIQakB9QBEEQBEEQBEEQBEEQakB+QBEEQRAEQRAEQRAEQagB+QFFEARBEARBEARBEAShBuQHFEEQBEEQBEEQBEEQhBqQH1AEQRAEQRAEQRAEQRBqQH5AEQRBEARBEARBEARBqIH/D/KLezHAIHCcAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 1400x800 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title you may run this to make sure your implementation for q is good\n",
        "\n",
        "x_0 = train[1][0].to(device)  # Initial image\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "T=1000\n",
        "ddpm = DDPM(T)\n",
        "\n",
        "for t in range(T):\n",
        "    t_tenser = torch.Tensor([t]).type(torch.int64)\n",
        "    x_t, noise = ddpm.q(x_0, t_tenser)\n",
        "    img = torch.squeeze(x_t).cpu()\n",
        "    if t % 100 == 0:\n",
        "      ax = plt.subplot(1, 10, t // 100 + 1)\n",
        "      ax.axis('off')\n",
        "      show_tensor_image(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOh7AyR5Zd8Z"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "lsWbEjaR2m6r"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name | Type | Params | Mode \n",
            "--------------------------------------\n",
            "0 | net  | UNet | 3.0 M  | train\n",
            "--------------------------------------\n",
            "3.0 M     Trainable params\n",
            "0         Non-trainable params\n",
            "3.0 M     Total params\n",
            "12.130    Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Net Num params:  3032449\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "941f09f9569745aba40dd3812759cd26",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "NameError",
          "evalue": "name 'latent_image_size' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[50], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m---> 19\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
            "Cell \u001b[0;32mIn[50], line 15\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m DDPM(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:579\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    573\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    575\u001b[0m     ckpt_path,\n\u001b[1;32m    576\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    577\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m )\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:986\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    991\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1028\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1027\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1028\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1030\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1057\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1054\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1057\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1059\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    390\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    395\u001b[0m )\n\u001b[0;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:311\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 311\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    314\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:411\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[48], line 93\u001b[0m, in \u001b[0;36mDDPM.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     90\u001b[0m x \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     91\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT, (BATCH_SIZE,), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 93\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_loss_in_epoch\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "Cell \u001b[0;32mIn[48], line 64\u001b[0m, in \u001b[0;36mDDPM.get_loss\u001b[0;34m(self, x_0, t)\u001b[0m\n\u001b[1;32m     62\u001b[0m noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(x_0)\u001b[38;5;241m.\u001b[39mto(x_0\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     63\u001b[0m x_t, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq(x_0, t)\n\u001b[0;32m---> 64\u001b[0m e_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mmse_loss(noise, e_t)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn[47], line 75\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     72\u001b[0m temb_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemb_2(t_emb)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Upsampling\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m up0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup0(latent_vec\u001b[38;5;241m.\u001b[39mview(latent_vec\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[43mlatent_image_size\u001b[49m, latent_image_size))\n\u001b[1;32m     76\u001b[0m up1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup1(up0 \u001b[38;5;241m+\u001b[39m temb_1[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m], down2)\n\u001b[1;32m     77\u001b[0m up2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup2(up1 \u001b[38;5;241m+\u001b[39m temb_2[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m], down1)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'latent_image_size' is not defined"
          ]
        }
      ],
      "source": [
        "def train_model(**kwargs):\n",
        "    # Create a PyTorch Lightning trainer with the generation callback\n",
        "    trainer = pl.Trainer(\n",
        "        default_root_dir=os.path.join(\"DDPM\"),\n",
        "        devices=1,\n",
        "        max_epochs=20,\n",
        "        callbacks=[\n",
        "            ModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"val_loss\"),\n",
        "            LearningRateMonitor(\"epoch\")\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model = DDPM(**kwargs)\n",
        "    model.train()\n",
        "    trainer.fit(model, train_dataloader, test_dataloader)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "model = train_model(T=1000).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQU8IOMAZFhf"
      },
      "outputs": [],
      "source": [
        "pl.seed_everything(1)\n",
        "sampled_data = model.sample(16)[:, -1]               # Get the last time step for each image\n",
        "show_imgs(sampled_data, nrows=4, grid=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URB1vjLsbCW0"
      },
      "source": [
        "## Autocomplete two images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1YhKj8MbCW1"
      },
      "outputs": [],
      "source": [
        "def complete_image(y=None, ymask=None, scale=1.):\n",
        "    # Implement here\n",
        "    # Hint: Use ddpm.get_x0_pred(x_t, t, e_t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL2uzCGEY-x0"
      },
      "source": [
        "# Bonus - Adding context\n",
        "<font color='red'>If you did the bonus, write it here</font><br/>\n",
        "\n",
        "MNIST is boring! Moreover, what is the point of generating samples without controling them? <br/>\n",
        "We will use a pretrained CLIP (Contrastive Language-Image Pre-Training). Given text, creates embedding. Our goal is to align image description and text embedding to each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaaUMZPat0pd"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet git+https://github.com/openai/CLIP.git\n",
        "\n",
        "import glob\n",
        "import csv\n",
        "from textwrap import wrap\n",
        "\n",
        "import clip\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDQzWRRMlLYT"
      },
      "source": [
        "## Load data\n",
        "Go to your <a href=\"https://www.kaggle.com/\">Kaggle</a> account and under the settings, generate new API token. <br/>\n",
        "This will export you a json file, which you will upload here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsQP__wzjHQp"
      },
      "outputs": [],
      "source": [
        "# The script expects you to upload JSON file to it!\n",
        "\n",
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets list\n",
        "! kaggle datasets download jessicali9530/celeba-dataset\n",
        "! unzip -q celeba-dataset.zip -d faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRoYygVmk2iU"
      },
      "outputs": [],
      "source": [
        "FACES_PATH = \"/content/faces/img_align_celeba/img_align_celeba\"\n",
        "\n",
        "for i in range(1,9):\n",
        "  img = Image.open(f'{FACES_PATH}/00000{i}.jpg')\n",
        "  plt.subplot(2, 4, i)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(img)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqU0DIjst5a7"
      },
      "source": [
        "## load pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5FzjTmiT42i"
      },
      "outputs": [],
      "source": [
        "clip.available_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8zRXdOAt9wH"
      },
      "outputs": [],
      "source": [
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\")\n",
        "clip_model.eval()\n",
        "CLIP_FEATURES = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvgB41A-naVP"
      },
      "source": [
        "## Intro to CLIP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_0bO2pR3OEO"
      },
      "source": [
        "Load image using CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTSyqKPpQSPq"
      },
      "outputs": [],
      "source": [
        "img = Image.open(f'{FACES_PATH}/000001.jpg')\n",
        "\n",
        "clip_imgs = torch.tensor(np.stack([clip_preprocess(img)])).to(device)\n",
        "print(\"After image clip preprocessing the size is \", clip_imgs.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqhqiEt530ml"
      },
      "source": [
        "Feature extractor of CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jNKDvgm32oS"
      },
      "outputs": [],
      "source": [
        "clip_img_encoding = clip_model.encode_image(clip_imgs)\n",
        "print(clip_img_encoding.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igam8MX75YwZ"
      },
      "source": [
        "Now, we want to see how to tokenize text and encoder it using clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KR_FILDZ5dFl"
      },
      "outputs": [],
      "source": [
        "text_list = [\n",
        "    \"An Angry man\",\n",
        "    \"Smiling bald person\",\n",
        "    \"Happy beautiful woman\"\n",
        "]\n",
        "text_tokens = clip.tokenize(text_list).to(device)\n",
        "print(\"Text tokens\")\n",
        "print(text_tokens[:,:10])\n",
        "print(\"----------------------------\")\n",
        "\n",
        "clip_text_encodings = clip_model.encode_text(text_tokens).float()\n",
        "print(\"For each text, encoding of 512 features \", clip_text_encodings.size())\n",
        "print(clip_text_encodings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgr5aa1b6kEy"
      },
      "source": [
        "In order to see which one of our text descriptions best describes the daisy, we can calculate the cosine similarity between the text encodings and the image encodings. <br/>\n",
        "We will load three flowers, give each its encoding and will compare to the texts above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykxsTFug7U2d"
      },
      "outputs": [],
      "source": [
        "def get_img_encodings(imgs):\n",
        "    processed_imgs = [clip_preprocess(img) for img in imgs]\n",
        "    clip_imgs = torch.tensor(np.stack(processed_imgs)).to(device)\n",
        "    clip_img_encodings = clip_model.encode_image(clip_imgs)\n",
        "    return clip_img_encodings\n",
        "\n",
        "imgs = [Image.open(f\"{FACES_PATH}/{i}.jpg\") for i in [\"000069\", \"000174\", \"000154\"]]\n",
        "for i, img in enumerate(imgs):\n",
        "    plt.subplot(1,3,i+1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f3bp5rW7xQU"
      },
      "outputs": [],
      "source": [
        "clip_img_encodings = get_img_encodings(imgs)    # torch.Tensor([3, 512])\n",
        "\n",
        "text_list = [\n",
        "    \"A surprised man with black hair\",\n",
        "    \"A woman smiling with red hair\",\n",
        "    \"A person with black glasses and wears black hat\"\n",
        "]\n",
        "\n",
        "text_tokens = clip.tokenize(text_list).to(device)\n",
        "clip_text_encodings = clip_model.encode_text(text_tokens)   # torch.Tensor([3, 512])\n",
        "\n",
        "clip_img_encodings /= clip_img_encodings.norm(dim=-1, keepdim=True)\n",
        "clip_text_encodings /= clip_text_encodings.norm(dim=-1, keepdim=True)\n",
        "\n",
        "similarity = clip_img_encodings @ clip_text_encodings.T\n",
        "\n",
        "print(similarity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-dFXoeX9aCR"
      },
      "source": [
        "Well, is there a match?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDcUIzbW87MA"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10, 10))\n",
        "gs = fig.add_gridspec(2, 3, wspace=.1, hspace=0)\n",
        "\n",
        "ax = fig.add_subplot(gs[1, :])\n",
        "plt.imshow(similarity.detach().cpu().numpy().T, vmin=0.1, vmax=0.3)\n",
        "\n",
        "labels = [ '\\n'.join(wrap(text, 20)) for text in text_list ]\n",
        "plt.yticks(range(len(text_tokens)), labels, fontsize=10)\n",
        "plt.xticks([])\n",
        "\n",
        "for x in range(similarity.shape[1]):\n",
        "    for y in range(similarity.shape[0]):\n",
        "        plt.text(x, y, f\"{similarity[x, y]:.2f}\", ha=\"center\", va=\"center\", size=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfVr3jwTdBVG"
      },
      "source": [
        "Collabory: CLIP gives the most similar encoding of image to the most similar encoding of text.\n",
        "Hence, we will train using the image encoding, but create new images using an encoding of text, hopefully it will work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaVgCZGFwuss"
      },
      "source": [
        "## Proccess the data using clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SMJJVCixBY-"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 32\n",
        "IMG_CH = 3\n",
        "BATCH_SIZE = 128\n",
        "CLIP_FEATURES = 512\n",
        "\n",
        "def crop_face(sample):\n",
        "  return sample[:, 9:(9+32),4:(4+32)]\n",
        "\n",
        "pre_transforms = transforms.Compose([\n",
        "    transforms.Resize((50, 40)),\n",
        "    transforms.ToTensor(),  # Scales data into [0,1]\n",
        "    crop_face,\n",
        "    transforms.Lambda(lambda t: (t * 2) - 1)  # Scale between [-1, 1]\n",
        "])\n",
        "\n",
        "random_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X268JiRQxMft"
      },
      "source": [
        "With our current resources, we DO NOT want to encode ~60000 imaegs. Download the csv file from the task pdf. It contains the file paths, along with its preprocessed CLIP data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1l_E-mIxKcq"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import csv\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, csv_path='clip_data.csv'):\n",
        "        self.imgs = []\n",
        "        self.labels = torch.empty(\n",
        "            len(data_paths), CLIP_FEATURES, dtype=torch.float, device=device\n",
        "        )\n",
        "\n",
        "        with open(csv_path, newline='') as csvfile:\n",
        "            reader = csv.reader(csvfile, delimiter=',')\n",
        "            for idx, row in enumerate(reader):\n",
        "                img = Image.open(row[0])\n",
        "                self.imgs.append(pre_transforms(img).to(device))\n",
        "                label = [float(x) for x in row[1:]]\n",
        "                self.labels[idx, :] = torch.FloatTensor(label).to(device)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return random_transforms(self.imgs[idx]), self.labels[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "data_paths = glob.glob(f'{FACES_PATH}/*.jpg', recursive=True)\n",
        "train_data = MyDataset(csv_path='/content/drive/MyDrive/clip.csv')\n",
        "\n",
        "faces_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3cmjqOhxcbz"
      },
      "source": [
        "## Modify the architercture to bring context into our model\n",
        "\n",
        "<img width=\"70%\" src=\"https://sharon.srworkspace.com/dgm/context1.png\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sO4FUfVnxcbz"
      },
      "outputs": [],
      "source": [
        "#@title Components of UNet\n",
        "import math\n",
        "\n",
        "class GELUConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, group_size):\n",
        "        super().__init__()\n",
        "        layers = [\n",
        "            nn.Conv2d(in_ch, out_ch, 3, 1, 1),\n",
        "            nn.GroupNorm(group_size, out_ch),\n",
        "            nn.GELU(),\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class RearrangePoolBlock(nn.Module):\n",
        "    def __init__(self, in_chs, group_size):\n",
        "        super().__init__()\n",
        "        self.rearrange = nn.MaxPool2d(2)\n",
        "        self.conv = GELUConvBlock(in_chs, in_chs, group_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rearrange(x)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class DownBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super(DownBlock, self).__init__()\n",
        "        layers = [\n",
        "            GELUConvBlock(in_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            RearrangePoolBlock(out_chs, group_size),\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super(UpBlock, self).__init__()\n",
        "        layers = [\n",
        "            nn.ConvTranspose2d(2 * in_chs, out_chs, 2, 2),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x = torch.cat((x, skip), 1)\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SinusoidalPositionEmbedBlock(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class EmbedBlock(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim):\n",
        "        super(EmbedBlock, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        layers = [\n",
        "            nn.Linear(input_dim, emb_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(emb_dim, emb_dim),\n",
        "            nn.Unflatten(1, (emb_dim, 1, 1)),\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.input_dim)\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class ResidualConvBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super().__init__()\n",
        "        self.conv1 = GELUConvBlock(in_chs, out_chs, group_size)\n",
        "        self.conv2 = GELUConvBlock(out_chs, out_chs, group_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv1(x)\n",
        "        x2 = self.conv2(x1)\n",
        "        out = x1 + x2\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTutNzEGxcb0"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, T):\n",
        "        super(UNet, self).__init__()\n",
        "        img_chs = IMG_CH\n",
        "        self.T = T\n",
        "        down_chs = (32, 64, 128)\n",
        "        up_chs = down_chs[::-1]  # Reverse of the down channels\n",
        "        latent_image_size = IMG_SIZE // 4 # 2 ** (len(down_chs) - 1)\n",
        "        t_embed_dim = 16\n",
        "        c_embed_dim=CLIP_FEATURES # New\n",
        "\n",
        "        small_group_size = 8 # New\n",
        "        big_group_size = 32 # New\n",
        "\n",
        "        # Inital convolution\n",
        "        self.down0 = ResidualConvBlock(IMG_CH, down_chs[0], small_group_size)\n",
        "\n",
        "        # Downsample\n",
        "        self.down1 = DownBlock(down_chs[0], down_chs[1], big_group_size)\n",
        "        self.down2 = DownBlock(down_chs[1], down_chs[2], big_group_size)\n",
        "        self.to_vec = nn.Sequential(nn.Flatten(), nn.GELU())\n",
        "\n",
        "        # Embeddings\n",
        "        self.dense_emb = nn.Sequential(\n",
        "            nn.Linear(down_chs[2] * latent_image_size**2, down_chs[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(down_chs[1], down_chs[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(down_chs[1], down_chs[2] * latent_image_size**2),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.sinusoidaltime = SinusoidalPositionEmbedBlock(t_embed_dim)\n",
        "        self.temb_1 = EmbedBlock(t_embed_dim, up_chs[0])\n",
        "        self.temb_2 = EmbedBlock(t_embed_dim, up_chs[1])\n",
        "        self.c_embed1 = EmbedBlock(c_embed_dim, up_chs[0])\n",
        "        self.c_embed2 = EmbedBlock(c_embed_dim, up_chs[1])\n",
        "\n",
        "        # Upsample\n",
        "        self.up0 = nn.Sequential(\n",
        "            nn.Unflatten(1, (up_chs[0], latent_image_size, latent_image_size)),\n",
        "            GELUConvBlock(up_chs[0], up_chs[0], big_group_size),\n",
        "        )\n",
        "        self.up1 = UpBlock(up_chs[0], up_chs[1], big_group_size)\n",
        "        self.up2 = UpBlock(up_chs[1], up_chs[2], big_group_size)\n",
        "\n",
        "        # Match output channels and one last concatenation\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Conv2d(2 * up_chs[-1], up_chs[-1], 3, 1, 1),\n",
        "            nn.GroupNorm(small_group_size, up_chs[-1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(up_chs[-1], IMG_CH, 3, 1, 1),\n",
        "        )\n",
        "\n",
        "        print(\"Num params: \", sum(p.numel() for p in self.parameters()))\n",
        "\n",
        "\n",
        "    def forward(self, x, t, c, c_mask):\n",
        "        # Implement here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbnTlwrNyFcR"
      },
      "outputs": [],
      "source": [
        "class DDPM(pl.LightningModule):\n",
        "    def __init__(self, T, method='cosine'):\n",
        "        super(DDPM, self).__init__()\n",
        "        self.T = T\n",
        "\n",
        "        epsilon=0.008\n",
        "        if method == 'cosine':\n",
        "            steps=torch.linspace(0,T,steps=T+1).to(device)\n",
        "            f_t=torch.cos(((steps/T+epsilon)/(1.0+epsilon))*math.pi*0.5)**2\n",
        "            self.Beta = torch.clip(1.0-f_t[1:]/f_t[:T], 0.0, 0.999)\n",
        "\n",
        "        elif method == 'linear':\n",
        "            self.Beta = torch.linspace(1e-4, 2e-2, T).to(device)\n",
        "\n",
        "        # Forward diffusion variables\n",
        "        self.a = 1.0 - self.Beta\n",
        "        self.a_bar = torch.cumprod(self.a, dim=0)\n",
        "\n",
        "        self.net = UNet(T)\n",
        "\n",
        "        # Logging\n",
        "        self.train_loss = []\n",
        "        self.train_loss_in_epoch = []\n",
        "\n",
        "    def q(self, x_0, t):\n",
        "        \"\"\"\n",
        "        Samples a new image from q\n",
        "        Returns the noise applied to an image at timestep t\n",
        "        x_0: the original image\n",
        "        t: timestep\n",
        "        \"\"\"\n",
        "        # Implement here\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def reverse_q(self, x_t, t, e_t):\n",
        "        # Implement here\n",
        "\n",
        "    def get_context_mask(self, c, drop_prob=0.1):\n",
        "        c_mask = torch.bernoulli(torch.ones_like(c).float() - drop_prob).to(device)\n",
        "        return c_mask\n",
        "\n",
        "    def get_loss(self, x_0, t, c):\n",
        "        \"\"\"\n",
        "        Returns the loss between the true noise and the predicted noise\n",
        "        x_0: the original image\n",
        "        t: timestep\n",
        "        \"\"\"\n",
        "        # Implement here\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, text_list, s=0.5):\n",
        "        # Implement here\n",
        "        return x_t      # (|text_list|, IMG_CH, IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "\n",
        "    # Lightning Configurations\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.99)\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, c = batch\n",
        "        t = torch.randint(0, self.T, (BATCH_SIZE,), device=device)\n",
        "\n",
        "        loss = self.get_loss(x, t, c)\n",
        "        self.train_loss_in_epoch.append(loss.item())\n",
        "        return loss\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        avg_loss = torch.tensor(self.train_loss_in_epoch).mean()\n",
        "        self.train_loss_in_epoch = []\n",
        "\n",
        "        self.log(\"train_loss\", avg_loss, prog_bar=True)\n",
        "        self.train_loss.append(avg_loss.detach().item())\n",
        "\n",
        "        print(f\"Epoch {self.current_epoch} | Loss: {avg_loss.detach().item()} \")\n",
        "        sampled_data = self.sample(text_list, s=1.)\n",
        "        show_imgs(sampled_data, grid=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo04CKG6NoSe"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idaf78ZGc_X0"
      },
      "source": [
        "This text list will be sampled at the end of each epoch, but does not affect the train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OC42EEI9yhLZ"
      },
      "outputs": [],
      "source": [
        "# Change me\n",
        "text_list = [\n",
        "    \"A man wearing a white hat\",\n",
        "    \"A woman in sun glasses\",\n",
        "    \"A man with green hair and a blue shirt\",\n",
        "    \"A sad woman with blue eyes\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29xs-z05P61u"
      },
      "outputs": [],
      "source": [
        "def train_model(**kwargs):\n",
        "    # Create a PyTorch Lightning trainer with the generation callback\n",
        "    trainer = pl.Trainer(\n",
        "        default_root_dir=os.path.join(\"DDPM\"),\n",
        "        devices=1,\n",
        "        max_epochs=50,\n",
        "        callbacks=[\n",
        "            ModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"train_loss\"),\n",
        "            LearningRateMonitor(\"epoch\")\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model = DDPM(**kwargs)\n",
        "    model.train()\n",
        "    trainer.fit(model, faces_dataloader)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "model = train_model(T=300, method='linear').to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkZDrwm0QEfb"
      },
      "source": [
        "## Final evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lR-7xU5RQENk"
      },
      "outputs": [],
      "source": [
        "text_list_new = [\n",
        "    \"A sad man with long hair\",\n",
        "    \"A smiling woman with green eyes\",\n",
        "]\n",
        "\n",
        "sampled_data = model.sample(text_list_new, s=0)\n",
        "show_imgs(sampled_data, grid=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "1MWoe14Ai3I-",
        "HA5GrP2zY9bI",
        "renhtvkaZBGA",
        "c5ErXXVQqSye",
        "MVdI8FjrZgRx",
        "wOh7AyR5Zd8Z",
        "URB1vjLsbCW0",
        "IL2uzCGEY-x0",
        "EDQzWRRMlLYT",
        "fqU0DIjst5a7",
        "qvgB41A-naVP",
        "HaVgCZGFwuss",
        "n3cmjqOhxcbz",
        "Zo04CKG6NoSe",
        "EkZDrwm0QEfb"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
